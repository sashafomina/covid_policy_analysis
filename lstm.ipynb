{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get raw input data for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beau/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (2,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# Get state policy data\n",
    "input_df = get_state_policy_data(fill=True)\n",
    "\n",
    "# Keep relevant columns\n",
    "policy_of_interest = POLICIES[0]\n",
    "input_df = input_df[['region_name', 'date', policy_of_interest]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename region_name to state\n",
    "input_df.rename(columns={'region_name': 'state'}, inplace=True)\n",
    "\n",
    "# Get US daily case data for 50 states\n",
    "us_state_daily = get_state_covid_daily_data()\n",
    "not_states = ['District of Columbia', 'Guam', 'Northern Mariana Islands','Puerto Rico', 'Virgin Islands']\n",
    "us_state_daily_50 =  us_state_daily[~(us_state_daily.state.isin(not_states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge daily cases with input df\n",
    "input_df = us_state_daily_50.merge(input_df, how='left', on=['state', 'date']).dropna().reset_index(drop=True)\n",
    "\n",
    "# Get cumulative cases and add as col to input df\n",
    "input_df['yesterdays_active_cases'] = get_yesterdays_active_cases(input_df, window=1)\n",
    "\n",
    "# Merge pop density with input df\n",
    "input_df = input_df.merge(get_pop_density_by_state_data(), on='state', how='left')\n",
    "\n",
    "# Merge wage data with input df\n",
    "input_df = input_df.merge(get_scaled_wages_data()[['state', 'scaled_median_income']], on=['state'], how='left')\n",
    "\n",
    "# Merge political data with input df\n",
    "input_df = input_df.merge(get_political_data(), on=['state'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to convert raw data to format needed for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset (input_df, time_steps = 7):\n",
    "    temp = input_df\n",
    "    e_xs, e_ys , d_xs, d_ys = [], [], [], []\n",
    "    for name_df, df in input_df.groupby(['state']):\n",
    "        y = df['cases'].to_numpy()\n",
    "        X = df.drop(columns=['state', 'deaths', 'date', 'cases']).to_numpy()\n",
    "        for i in range(len(X)-time_steps):\n",
    "            if (i < len(X) - (time_steps * 2)):\n",
    "                e_v = X[i:i+time_steps, :]\n",
    "                e_xs.append(e_v)\n",
    "                e_ys.append(y[i+time_steps])\n",
    "\n",
    "                d_v = X[(i+time_steps):(i+(time_steps*2)), 0:2]\n",
    "                d_xs.append(d_v)\n",
    "                d_ys.append(y[(i+(time_steps*2))])\n",
    "            \n",
    "    return np.array(e_xs), np.array(e_ys), np.array(d_xs), np.array(d_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab time window to split training and test data\n",
    "date_start = \"2020-01-01\"\n",
    "date_end = \"2020-12-31\"\n",
    "train_df = input_df.loc[(input_df['date'] >= date_start) & \n",
    "                        (input_df['date'] <= date_end)].copy().reset_index(drop=True)\n",
    "test_df = input_df.loc[(input_df['date'] > date_end)].copy().reset_index(drop=True)\n",
    "\n",
    "# Get input and output for training\n",
    "e_x_train_np, e_y_train_np, d_x_train_np, d_y_train_np = create_dataset(train_df)\n",
    "\n",
    "# Get input and output for testing\n",
    "e_x_test_np, e_y_test_np, d_x_test_np, d_y_test_np = create_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dual Purpose Encoder/Decoder Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ED_LSTM(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ED_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define the RNN layer, LSTM\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            hidden_dim, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, input_sequence, states):\n",
    "        # Call the LSTM unit\n",
    "        lstm_out, state_h, state_c = self.lstm(input_sequence, initial_state=states)\n",
    "        # Dense layer to predict output token\n",
    "        output = self.dense(lstm_out)\n",
    "        \n",
    "        return output, state_h, state_c\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        # Return a all 0s initial states\n",
    "        return (tf.zeros([batch_size, self.hidden_dim]),\n",
    "                tf.zeros([batch_size, self.hidden_dim]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions for LSTM encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_train_step(input_seq, target, en_initial_states, optimizer):\n",
    "    ''' A training step, train a batch of the data and return the loss value reached\n",
    "        Input:\n",
    "        - input_seq: array of integers, shape [batch_size, seq_len, num_features].\n",
    "            the input sequence\n",
    "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, 1].\n",
    "            the target seq, our target sequence\n",
    "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, 1].\n",
    "            the input sequence to the decoder, we use Teacher Forcing\n",
    "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
    "            the initial state of the encoder\n",
    "        - optimizer: a tf.keras.optimizers.\n",
    "        Output:\n",
    "        - loss: loss value\n",
    "        \n",
    "    '''\n",
    "    # Networkâ€™s computations need to be put under tf.GradientTape() to keep track of gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the encoder outputs\n",
    "        en_outputs = encoder(input_seq, en_initial_states)\n",
    "        output = en_outputs[0]\n",
    "        # Calculate the loss function\n",
    "        loss_fun = keras.losses.MeanSquaredError()\n",
    "        loss = loss_fun(target, output)\n",
    "\n",
    "    variables = encoder.trainable_variables\n",
    "    # Calculate the gradients for the variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # Apply the gradients and update the optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_train(encoder, dataset, n_epochs, batch_size, optimizer):\n",
    "    losses = []\n",
    "\n",
    "    X = dataset[0]\n",
    "    y = dataset[1]\n",
    "    for e in range(n_epochs):\n",
    "        # Get the initial time\n",
    "        start = time.time()\n",
    "        # Get the initial state for the encoder\n",
    "        en_initial_states = encoder.init_states(batch_size)\n",
    "        # Generate a random ordering for the data\n",
    "        perm = np.random.permutation(range(0, len(X)))\n",
    "        X = X[perm]\n",
    "        y = y[perm]\n",
    "        # For every batch data\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            if i + batch_size >= len(X):\n",
    "                en_initial_states = encoder.init_states((len(X) - i))\n",
    "                input_seq = tf.constant(X[i:(len(X))])\n",
    "                target = tf.reshape(tf.constant(y[i:(len(X))]), ((len(X) - i), 1))\n",
    "            else:\n",
    "                input_seq = tf.constant(X[i:i+batch_size])\n",
    "                target = tf.reshape(tf.constant(y[i:i+batch_size]), (batch_size, 1))\n",
    "                \n",
    "            # Train and get the loss value \n",
    "            loss = encoder_train_step(input_seq, target, en_initial_states, optimizer)\n",
    "        \n",
    "            if i % 1000 == 0:\n",
    "                # Store the loss and accuracy values\n",
    "                losses.append(loss)\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(e + 1, i, loss.numpy()))\n",
    "    \n",
    "        print('Time taken for 1 epoch {:.4f} sec\\n'.format(time.time() - start))\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Encoder LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the encoder LSTM, we will train on input sequences where each sequence represents a set of consecutive days, and each day will be represented by a set of features corresponding to that day (e.g. yesterday's daily case number, political index, policy strictness, etc.). After the model is trained via minimizing mean squared error or mean absolute error, we will do a run through all the training data so as to capture the hidden state and cell state of the encoder LSTM after being fed a particular sequence. These hidden states and cell states will be used to initialize the encoder LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ED_LSTM(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer ed_lstm_49 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1 Batch 0 Loss 32313.9590\n",
      "Epoch 1 Batch 4000 Loss 804171.7500\n",
      "Epoch 1 Batch 8000 Loss 929572.1250\n",
      "Epoch 1 Batch 12000 Loss 4058788.5000\n",
      "Time taken for 1 epoch 6.9394 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 22409.6113\n",
      "Epoch 2 Batch 4000 Loss 743684.2500\n",
      "Epoch 2 Batch 8000 Loss 880279.6250\n",
      "Epoch 2 Batch 12000 Loss 3968513.0000\n",
      "Time taken for 1 epoch 6.6509 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 17497.5430\n",
      "Epoch 3 Batch 4000 Loss 706827.2500\n",
      "Epoch 3 Batch 8000 Loss 840823.5000\n",
      "Epoch 3 Batch 12000 Loss 3885023.5000\n",
      "Time taken for 1 epoch 6.5971 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 13726.9023\n",
      "Epoch 4 Batch 4000 Loss 672883.1250\n",
      "Epoch 4 Batch 8000 Loss 803790.2500\n",
      "Epoch 4 Batch 12000 Loss 3806958.5000\n",
      "Time taken for 1 epoch 6.5521 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 10687.1709\n",
      "Epoch 5 Batch 4000 Loss 641194.6875\n",
      "Epoch 5 Batch 8000 Loss 769234.5000\n",
      "Epoch 5 Batch 12000 Loss 3731489.7500\n",
      "Time taken for 1 epoch 6.6721 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 8269.4443\n",
      "Epoch 6 Batch 4000 Loss 610360.1875\n",
      "Epoch 6 Batch 8000 Loss 735484.3750\n",
      "Epoch 6 Batch 12000 Loss 3657535.0000\n",
      "Time taken for 1 epoch 6.9052 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 6612.3794\n",
      "Epoch 7 Batch 4000 Loss 581350.0625\n",
      "Epoch 7 Batch 8000 Loss 703503.2500\n",
      "Epoch 7 Batch 12000 Loss 3586255.2500\n",
      "Time taken for 1 epoch 6.8520 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 5249.1553\n",
      "Epoch 8 Batch 4000 Loss 552933.5000\n",
      "Epoch 8 Batch 8000 Loss 671487.3125\n",
      "Epoch 8 Batch 12000 Loss 3513218.7500\n",
      "Time taken for 1 epoch 6.7685 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 4397.8145\n",
      "Epoch 9 Batch 4000 Loss 525199.1250\n",
      "Epoch 9 Batch 8000 Loss 641156.9375\n",
      "Epoch 9 Batch 12000 Loss 3443940.5000\n",
      "Time taken for 1 epoch 6.6926 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 3746.1160\n",
      "Epoch 10 Batch 4000 Loss 499141.7188\n",
      "Epoch 10 Batch 8000 Loss 612152.0000\n",
      "Epoch 10 Batch 12000 Loss 3375470.0000\n",
      "Time taken for 1 epoch 6.5472 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 3641.7686\n",
      "Epoch 11 Batch 4000 Loss 472896.3125\n",
      "Epoch 11 Batch 8000 Loss 582591.0625\n",
      "Epoch 11 Batch 12000 Loss 3305948.0000\n",
      "Time taken for 1 epoch 6.5512 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 3882.3569\n",
      "Epoch 12 Batch 4000 Loss 448028.9062\n",
      "Epoch 12 Batch 8000 Loss 554779.6250\n",
      "Epoch 12 Batch 12000 Loss 3238593.0000\n",
      "Time taken for 1 epoch 6.4264 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 3992.0630\n",
      "Epoch 13 Batch 4000 Loss 423712.0000\n",
      "Epoch 13 Batch 8000 Loss 527330.5625\n",
      "Epoch 13 Batch 12000 Loss 3172139.0000\n",
      "Time taken for 1 epoch 6.4511 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 4447.0244\n",
      "Epoch 14 Batch 4000 Loss 400642.9375\n",
      "Epoch 14 Batch 8000 Loss 501321.1875\n",
      "Epoch 14 Batch 12000 Loss 3107687.5000\n",
      "Time taken for 1 epoch 6.4456 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 4856.4585\n",
      "Epoch 15 Batch 4000 Loss 378589.8125\n",
      "Epoch 15 Batch 8000 Loss 476330.9375\n",
      "Epoch 15 Batch 12000 Loss 3044649.2500\n",
      "Time taken for 1 epoch 6.6216 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 5425.2285\n",
      "Epoch 16 Batch 4000 Loss 357480.0312\n",
      "Epoch 16 Batch 8000 Loss 452302.6875\n",
      "Epoch 16 Batch 12000 Loss 3005524.0000\n",
      "Time taken for 1 epoch 6.4816 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 5513.7168\n",
      "Epoch 17 Batch 4000 Loss 337380.9375\n",
      "Epoch 17 Batch 8000 Loss 429329.8750\n",
      "Epoch 17 Batch 12000 Loss 2922957.5000\n",
      "Time taken for 1 epoch 6.6747 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 5612.1572\n",
      "Epoch 18 Batch 4000 Loss 317998.7500\n",
      "Epoch 18 Batch 8000 Loss 407059.7500\n",
      "Epoch 18 Batch 12000 Loss 2863578.5000\n",
      "Time taken for 1 epoch 6.9326 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 5090.7441\n",
      "Epoch 19 Batch 4000 Loss 299401.5000\n",
      "Epoch 19 Batch 8000 Loss 385596.2500\n",
      "Epoch 19 Batch 12000 Loss 2805363.7500\n",
      "Time taken for 1 epoch 6.8615 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 5556.2852\n",
      "Epoch 20 Batch 4000 Loss 281633.9375\n",
      "Epoch 20 Batch 8000 Loss 364839.6562\n",
      "Epoch 20 Batch 12000 Loss 2747400.5000\n",
      "Time taken for 1 epoch 7.0591 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 5055.3276\n",
      "Epoch 21 Batch 4000 Loss 264210.1562\n",
      "Epoch 21 Batch 8000 Loss 344603.7500\n",
      "Epoch 21 Batch 12000 Loss 2690482.2500\n",
      "Time taken for 1 epoch 6.4797 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 4785.3799\n",
      "Epoch 22 Batch 4000 Loss 247590.5469\n",
      "Epoch 22 Batch 8000 Loss 325092.7812\n",
      "Epoch 22 Batch 12000 Loss 2634063.0000\n",
      "Time taken for 1 epoch 6.6299 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 4663.4482\n",
      "Epoch 23 Batch 4000 Loss 231889.3750\n",
      "Epoch 23 Batch 8000 Loss 306572.1875\n",
      "Epoch 23 Batch 12000 Loss 2579365.5000\n",
      "Time taken for 1 epoch 6.3962 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 4556.4902\n",
      "Epoch 24 Batch 4000 Loss 216970.7031\n",
      "Epoch 24 Batch 8000 Loss 288843.1250\n",
      "Epoch 24 Batch 12000 Loss 2525732.5000\n",
      "Time taken for 1 epoch 6.7105 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 4685.9707\n",
      "Epoch 25 Batch 4000 Loss 202793.2812\n",
      "Epoch 25 Batch 8000 Loss 271871.0312\n",
      "Epoch 25 Batch 12000 Loss 2473184.0000\n",
      "Time taken for 1 epoch 7.0842 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 5125.0156\n",
      "Epoch 26 Batch 4000 Loss 189353.2188\n",
      "Epoch 26 Batch 8000 Loss 255648.2188\n",
      "Epoch 26 Batch 12000 Loss 2421206.0000\n",
      "Time taken for 1 epoch 6.7721 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 4860.1045\n",
      "Epoch 27 Batch 4000 Loss 176373.2812\n",
      "Epoch 27 Batch 8000 Loss 239835.2344\n",
      "Epoch 27 Batch 12000 Loss 2369875.2500\n",
      "Time taken for 1 epoch 6.4953 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 4669.5840\n",
      "Epoch 28 Batch 4000 Loss 164182.1094\n",
      "Epoch 28 Batch 8000 Loss 224872.0312\n",
      "Epoch 28 Batch 12000 Loss 2319718.0000\n",
      "Time taken for 1 epoch 6.4590 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 4798.8916\n",
      "Epoch 29 Batch 4000 Loss 152690.7500\n",
      "Epoch 29 Batch 8000 Loss 210628.2500\n",
      "Epoch 29 Batch 12000 Loss 2270509.5000\n",
      "Time taken for 1 epoch 6.5827 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 4600.3262\n",
      "Epoch 30 Batch 4000 Loss 141878.8750\n",
      "Epoch 30 Batch 8000 Loss 197080.1875\n",
      "Epoch 30 Batch 12000 Loss 2224649.7500\n",
      "Time taken for 1 epoch 6.5275 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 4923.6973\n",
      "Epoch 31 Batch 4000 Loss 131370.1094\n",
      "Epoch 31 Batch 8000 Loss 183728.4844\n",
      "Epoch 31 Batch 12000 Loss 2172998.5000\n",
      "Time taken for 1 epoch 6.3967 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 4538.3706\n",
      "Epoch 32 Batch 4000 Loss 121751.1797\n",
      "Epoch 32 Batch 8000 Loss 171373.7188\n",
      "Epoch 32 Batch 12000 Loss 2126047.0000\n",
      "Time taken for 1 epoch 6.4224 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 4256.8745\n",
      "Epoch 33 Batch 4000 Loss 112814.0547\n",
      "Epoch 33 Batch 8000 Loss 159732.2812\n",
      "Epoch 33 Batch 12000 Loss 2097813.2500\n",
      "Time taken for 1 epoch 6.5740 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 4397.9917\n",
      "Epoch 34 Batch 4000 Loss 104515.0547\n",
      "Epoch 34 Batch 8000 Loss 148733.9219\n",
      "Epoch 34 Batch 12000 Loss 2035088.7500\n",
      "Time taken for 1 epoch 6.8425 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 4386.3115\n",
      "Epoch 35 Batch 4000 Loss 96827.5781\n",
      "Epoch 35 Batch 8000 Loss 138360.6875\n",
      "Epoch 35 Batch 12000 Loss 1991012.5000\n",
      "Time taken for 1 epoch 6.5441 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 4071.5723\n",
      "Epoch 36 Batch 4000 Loss 89746.3516\n",
      "Epoch 36 Batch 8000 Loss 128215.1719\n",
      "Epoch 36 Batch 12000 Loss 1945254.1250\n",
      "Time taken for 1 epoch 6.7555 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 4529.4331\n",
      "Epoch 37 Batch 4000 Loss 82861.1953\n",
      "Epoch 37 Batch 8000 Loss 118886.4375\n",
      "Epoch 37 Batch 12000 Loss 1902004.2500\n",
      "Time taken for 1 epoch 6.5662 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 4882.3730\n",
      "Epoch 38 Batch 4000 Loss 80058.1094\n",
      "Epoch 38 Batch 8000 Loss 110257.8672\n",
      "Epoch 38 Batch 12000 Loss 1861709.7500\n",
      "Time taken for 1 epoch 6.4738 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 4867.6309\n",
      "Epoch 39 Batch 4000 Loss 72048.7344\n",
      "Epoch 39 Batch 8000 Loss 102369.3906\n",
      "Epoch 39 Batch 12000 Loss 1818305.2500\n",
      "Time taken for 1 epoch 6.4655 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 4096.6567\n",
      "Epoch 40 Batch 4000 Loss 67214.3281\n",
      "Epoch 40 Batch 8000 Loss 95164.6016\n",
      "Epoch 40 Batch 12000 Loss 1777694.7500\n",
      "Time taken for 1 epoch 6.3778 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 3901.3242\n",
      "Epoch 41 Batch 4000 Loss 65761.1875\n",
      "Epoch 41 Batch 8000 Loss 88068.9688\n",
      "Epoch 41 Batch 12000 Loss 1737312.6250\n",
      "Time taken for 1 epoch 6.3981 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 4338.7451\n",
      "Epoch 42 Batch 4000 Loss 65037.5938\n",
      "Epoch 42 Batch 8000 Loss 81762.5156\n",
      "Epoch 42 Batch 12000 Loss 1697629.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch 6.7807 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 5329.7974\n",
      "Epoch 43 Batch 4000 Loss 61556.4062\n",
      "Epoch 43 Batch 8000 Loss 75982.0000\n",
      "Epoch 43 Batch 12000 Loss 1658918.7500\n",
      "Time taken for 1 epoch 6.7046 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 5196.9375\n",
      "Epoch 44 Batch 4000 Loss 54397.4961\n",
      "Epoch 44 Batch 8000 Loss 70662.7188\n",
      "Epoch 44 Batch 12000 Loss 1620983.5000\n",
      "Time taken for 1 epoch 6.4416 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 4940.8921\n",
      "Epoch 45 Batch 4000 Loss 55542.3164\n",
      "Epoch 45 Batch 8000 Loss 64826.6172\n",
      "Epoch 45 Batch 12000 Loss 1583821.1250\n",
      "Time taken for 1 epoch 6.4084 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 4977.0791\n",
      "Epoch 46 Batch 4000 Loss 56221.2969\n",
      "Epoch 46 Batch 8000 Loss 61162.4531\n",
      "Epoch 46 Batch 12000 Loss 1547408.5000\n",
      "Time taken for 1 epoch 6.4302 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 4979.5527\n",
      "Epoch 47 Batch 4000 Loss 55767.4844\n",
      "Epoch 47 Batch 8000 Loss 57212.4297\n",
      "Epoch 47 Batch 12000 Loss 1511654.7500\n",
      "Time taken for 1 epoch 6.3673 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 5138.8511\n",
      "Epoch 48 Batch 4000 Loss 55385.7656\n",
      "Epoch 48 Batch 8000 Loss 53121.2930\n",
      "Epoch 48 Batch 12000 Loss 1475009.7500\n",
      "Time taken for 1 epoch 6.4246 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 5077.2354\n",
      "Epoch 49 Batch 4000 Loss 59221.1406\n",
      "Epoch 49 Batch 8000 Loss 50990.4688\n",
      "Epoch 49 Batch 12000 Loss 1439541.1250\n",
      "Time taken for 1 epoch 6.3443 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 4492.4609\n",
      "Epoch 50 Batch 4000 Loss 54633.7852\n",
      "Epoch 50 Batch 8000 Loss 47894.4766\n",
      "Epoch 50 Batch 12000 Loss 1405197.7500\n",
      "Time taken for 1 epoch 6.3661 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 5035.4883\n",
      "Epoch 51 Batch 4000 Loss 57625.5547\n",
      "Epoch 51 Batch 8000 Loss 44205.7812\n",
      "Epoch 51 Batch 12000 Loss 1371681.5000\n",
      "Time taken for 1 epoch 6.7636 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 4714.3994\n",
      "Epoch 52 Batch 4000 Loss 61258.6484\n",
      "Epoch 52 Batch 8000 Loss 43908.2422\n",
      "Epoch 52 Batch 12000 Loss 1338802.5000\n",
      "Time taken for 1 epoch 6.8340 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 4810.4785\n",
      "Epoch 53 Batch 4000 Loss 59842.2773\n",
      "Epoch 53 Batch 8000 Loss 43917.8633\n",
      "Epoch 53 Batch 12000 Loss 1326781.8750\n",
      "Time taken for 1 epoch 6.4346 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 4492.2734\n",
      "Epoch 54 Batch 4000 Loss 53832.3281\n",
      "Epoch 54 Batch 8000 Loss 41282.6094\n",
      "Epoch 54 Batch 12000 Loss 1295095.0000\n",
      "Time taken for 1 epoch 6.6455 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 4489.1978\n",
      "Epoch 55 Batch 4000 Loss 61479.7812\n",
      "Epoch 55 Batch 8000 Loss 39478.2812\n",
      "Epoch 55 Batch 12000 Loss 1264115.1250\n",
      "Time taken for 1 epoch 6.8878 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 4551.2510\n",
      "Epoch 56 Batch 4000 Loss 60765.8945\n",
      "Epoch 56 Batch 8000 Loss 41038.0625\n",
      "Epoch 56 Batch 12000 Loss 1214232.8750\n",
      "Time taken for 1 epoch 7.1143 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 4214.2031\n",
      "Epoch 57 Batch 4000 Loss 54271.7812\n",
      "Epoch 57 Batch 8000 Loss 38904.4180\n",
      "Epoch 57 Batch 12000 Loss 1182876.2500\n",
      "Time taken for 1 epoch 6.4471 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 3810.4453\n",
      "Epoch 58 Batch 4000 Loss 60503.9023\n",
      "Epoch 58 Batch 8000 Loss 40563.2773\n",
      "Epoch 58 Batch 12000 Loss 1153486.2500\n",
      "Time taken for 1 epoch 6.5260 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 3719.6309\n",
      "Epoch 59 Batch 4000 Loss 63275.7852\n",
      "Epoch 59 Batch 8000 Loss 39648.1328\n",
      "Epoch 59 Batch 12000 Loss 1124814.0000\n",
      "Time taken for 1 epoch 6.9408 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 4925.9834\n",
      "Epoch 60 Batch 4000 Loss 66525.6250\n",
      "Epoch 60 Batch 8000 Loss 41022.0156\n",
      "Epoch 60 Batch 12000 Loss 1096796.0000\n",
      "Time taken for 1 epoch 6.6846 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 4360.1304\n",
      "Epoch 61 Batch 4000 Loss 58552.9609\n",
      "Epoch 61 Batch 8000 Loss 40234.4727\n",
      "Epoch 61 Batch 12000 Loss 1066710.8750\n",
      "Time taken for 1 epoch 6.4954 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 4364.1709\n",
      "Epoch 62 Batch 4000 Loss 56217.3203\n",
      "Epoch 62 Batch 8000 Loss 42957.0586\n",
      "Epoch 62 Batch 12000 Loss 1039587.1250\n",
      "Time taken for 1 epoch 6.4347 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 4250.5415\n",
      "Epoch 63 Batch 4000 Loss 66295.0781\n",
      "Epoch 63 Batch 8000 Loss 41991.8516\n",
      "Epoch 63 Batch 12000 Loss 1053320.6250\n",
      "Time taken for 1 epoch 6.3952 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 4542.6694\n",
      "Epoch 64 Batch 4000 Loss 70820.7578\n",
      "Epoch 64 Batch 8000 Loss 38984.4375\n",
      "Epoch 64 Batch 12000 Loss 1031157.0000\n",
      "Time taken for 1 epoch 6.5773 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 4905.4678\n",
      "Epoch 65 Batch 4000 Loss 62982.2891\n",
      "Epoch 65 Batch 8000 Loss 43548.8281\n",
      "Epoch 65 Batch 12000 Loss 1004892.6250\n",
      "Time taken for 1 epoch 6.3880 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 5107.3965\n",
      "Epoch 66 Batch 4000 Loss 55437.6719\n",
      "Epoch 66 Batch 8000 Loss 42624.9453\n",
      "Epoch 66 Batch 12000 Loss 983256.5000\n",
      "Time taken for 1 epoch 6.7856 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 4993.4873\n",
      "Epoch 67 Batch 4000 Loss 59361.7500\n",
      "Epoch 67 Batch 8000 Loss 42906.6250\n",
      "Epoch 67 Batch 12000 Loss 954453.7500\n",
      "Time taken for 1 epoch 6.3382 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 4950.9287\n",
      "Epoch 68 Batch 4000 Loss 61359.9648\n",
      "Epoch 68 Batch 8000 Loss 42977.0625\n",
      "Epoch 68 Batch 12000 Loss 949697.0000\n",
      "Time taken for 1 epoch 6.4232 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 5062.8857\n",
      "Epoch 69 Batch 4000 Loss 70561.4531\n",
      "Epoch 69 Batch 8000 Loss 39128.4297\n",
      "Epoch 69 Batch 12000 Loss 924917.8750\n",
      "Time taken for 1 epoch 6.3780 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 4697.8701\n",
      "Epoch 70 Batch 4000 Loss 60969.1250\n",
      "Epoch 70 Batch 8000 Loss 45059.9453\n",
      "Epoch 70 Batch 12000 Loss 903056.7500\n",
      "Time taken for 1 epoch 6.4191 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 4484.3335\n",
      "Epoch 71 Batch 4000 Loss 67995.5000\n",
      "Epoch 71 Batch 8000 Loss 43604.1719\n",
      "Epoch 71 Batch 12000 Loss 898185.0625\n",
      "Time taken for 1 epoch 6.3721 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 4568.2954\n",
      "Epoch 72 Batch 4000 Loss 71260.6016\n",
      "Epoch 72 Batch 8000 Loss 50134.5039\n",
      "Epoch 72 Batch 12000 Loss 837078.9375\n",
      "Time taken for 1 epoch 6.6450 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 4968.3027\n",
      "Epoch 73 Batch 4000 Loss 71149.8125\n",
      "Epoch 73 Batch 8000 Loss 51223.7969\n",
      "Epoch 73 Batch 12000 Loss 834376.3750\n",
      "Time taken for 1 epoch 6.4082 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 4472.2520\n",
      "Epoch 74 Batch 4000 Loss 65805.6406\n",
      "Epoch 74 Batch 8000 Loss 46244.5156\n",
      "Epoch 74 Batch 12000 Loss 812482.6875\n",
      "Time taken for 1 epoch 6.4612 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 4972.8672\n",
      "Epoch 75 Batch 4000 Loss 59410.7109\n",
      "Epoch 75 Batch 8000 Loss 49423.9883\n",
      "Epoch 75 Batch 12000 Loss 791254.0000\n",
      "Time taken for 1 epoch 6.5162 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 4235.2021\n",
      "Epoch 76 Batch 4000 Loss 61198.1523\n",
      "Epoch 76 Batch 8000 Loss 50931.2422\n",
      "Epoch 76 Batch 12000 Loss 770478.2500\n",
      "Time taken for 1 epoch 6.5061 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 4659.6982\n",
      "Epoch 77 Batch 4000 Loss 57223.5664\n",
      "Epoch 77 Batch 8000 Loss 42616.4141\n",
      "Epoch 77 Batch 12000 Loss 750255.4375\n",
      "Time taken for 1 epoch 6.4934 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 4762.9951\n",
      "Epoch 78 Batch 4000 Loss 61200.0078\n",
      "Epoch 78 Batch 8000 Loss 47155.0781\n",
      "Epoch 78 Batch 12000 Loss 735001.3750\n",
      "Time taken for 1 epoch 6.4350 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 4935.4219\n",
      "Epoch 79 Batch 4000 Loss 56972.0781\n",
      "Epoch 79 Batch 8000 Loss 46193.7188\n",
      "Epoch 79 Batch 12000 Loss 716674.1250\n",
      "Time taken for 1 epoch 6.4869 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 4693.6270\n",
      "Epoch 80 Batch 4000 Loss 55791.6758\n",
      "Epoch 80 Batch 8000 Loss 44484.1523\n",
      "Epoch 80 Batch 12000 Loss 696629.1875\n",
      "Time taken for 1 epoch 7.3077 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 4040.5056\n",
      "Epoch 81 Batch 4000 Loss 75216.2344\n",
      "Epoch 81 Batch 8000 Loss 46480.3984\n",
      "Epoch 81 Batch 12000 Loss 673700.4375\n",
      "Time taken for 1 epoch 6.8413 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 4482.5879\n",
      "Epoch 82 Batch 4000 Loss 121049.3750\n",
      "Epoch 82 Batch 8000 Loss 48676.1445\n",
      "Epoch 82 Batch 12000 Loss 655755.9375\n",
      "Time taken for 1 epoch 6.9572 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 4756.2251\n",
      "Epoch 83 Batch 4000 Loss 56413.3047\n",
      "Epoch 83 Batch 8000 Loss 48441.7383\n",
      "Epoch 83 Batch 12000 Loss 638210.0000\n",
      "Time taken for 1 epoch 6.5326 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 4392.2349\n",
      "Epoch 84 Batch 4000 Loss 60182.7734\n",
      "Epoch 84 Batch 8000 Loss 44029.8281\n",
      "Epoch 84 Batch 12000 Loss 638028.1250\n",
      "Time taken for 1 epoch 6.3714 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 4383.1079\n",
      "Epoch 85 Batch 4000 Loss 58484.0039\n",
      "Epoch 85 Batch 8000 Loss 49898.0469\n",
      "Epoch 85 Batch 12000 Loss 621171.2500\n",
      "Time taken for 1 epoch 6.6267 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 4732.5269\n",
      "Epoch 86 Batch 4000 Loss 58762.4219\n",
      "Epoch 86 Batch 8000 Loss 47381.7812\n",
      "Epoch 86 Batch 12000 Loss 642777.6250\n",
      "Time taken for 1 epoch 6.6185 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 5246.3535\n",
      "Epoch 87 Batch 4000 Loss 81996.5312\n",
      "Epoch 87 Batch 8000 Loss 44515.4492\n",
      "Epoch 87 Batch 12000 Loss 589831.0000\n",
      "Time taken for 1 epoch 6.4831 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 4743.3169\n",
      "Epoch 88 Batch 4000 Loss 63184.9844\n",
      "Epoch 88 Batch 8000 Loss 47366.3906\n",
      "Epoch 88 Batch 12000 Loss 565234.2500\n",
      "Time taken for 1 epoch 6.4280 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 4972.3691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Batch 4000 Loss 69592.8281\n",
      "Epoch 89 Batch 8000 Loss 45532.7500\n",
      "Epoch 89 Batch 12000 Loss 550317.8125\n",
      "Time taken for 1 epoch 6.4118 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 5559.5635\n",
      "Epoch 90 Batch 4000 Loss 146158.9844\n",
      "Epoch 90 Batch 8000 Loss 48431.3711\n",
      "Epoch 90 Batch 12000 Loss 532128.1250\n",
      "Time taken for 1 epoch 6.4602 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 4978.5459\n",
      "Epoch 91 Batch 4000 Loss 148680.7500\n",
      "Epoch 91 Batch 8000 Loss 48876.1211\n",
      "Epoch 91 Batch 12000 Loss 518287.5625\n",
      "Time taken for 1 epoch 6.5047 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 4586.0088\n",
      "Epoch 92 Batch 4000 Loss 73718.9766\n",
      "Epoch 92 Batch 8000 Loss 47711.8984\n",
      "Epoch 92 Batch 12000 Loss 520121.2812\n",
      "Time taken for 1 epoch 6.7458 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 5155.9502\n",
      "Epoch 93 Batch 4000 Loss 76612.3594\n",
      "Epoch 93 Batch 8000 Loss 41817.1016\n",
      "Epoch 93 Batch 12000 Loss 493813.3125\n",
      "Time taken for 1 epoch 6.4468 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 4759.4521\n",
      "Epoch 94 Batch 4000 Loss 68059.1719\n",
      "Epoch 94 Batch 8000 Loss 46877.5703\n",
      "Epoch 94 Batch 12000 Loss 494293.0938\n",
      "Time taken for 1 epoch 6.3943 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 5254.9126\n",
      "Epoch 95 Batch 4000 Loss 52804.9727\n",
      "Epoch 95 Batch 8000 Loss 50300.7031\n",
      "Epoch 95 Batch 12000 Loss 501475.0000\n",
      "Time taken for 1 epoch 6.4069 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 4380.6421\n",
      "Epoch 96 Batch 4000 Loss 81142.9844\n",
      "Epoch 96 Batch 8000 Loss 47353.4883\n",
      "Epoch 96 Batch 12000 Loss 478072.0938\n",
      "Time taken for 1 epoch 6.3567 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 4320.1733\n",
      "Epoch 97 Batch 4000 Loss 68389.3203\n",
      "Epoch 97 Batch 8000 Loss 65005.9062\n",
      "Epoch 97 Batch 12000 Loss 496970.8125\n",
      "Time taken for 1 epoch 6.4122 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 5378.0049\n",
      "Epoch 98 Batch 4000 Loss 96630.6562\n",
      "Epoch 98 Batch 8000 Loss 49654.3906\n",
      "Epoch 98 Batch 12000 Loss 468699.7500\n",
      "Time taken for 1 epoch 6.6648 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 4239.8193\n",
      "Epoch 99 Batch 4000 Loss 124381.0703\n",
      "Epoch 99 Batch 8000 Loss 54530.6953\n",
      "Epoch 99 Batch 12000 Loss 467128.7500\n",
      "Time taken for 1 epoch 6.9141 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 5163.8877\n",
      "Epoch 100 Batch 4000 Loss 69750.8672\n",
      "Epoch 100 Batch 8000 Loss 47180.0625\n",
      "Epoch 100 Batch 12000 Loss 475774.6562\n",
      "Time taken for 1 epoch 7.0541 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = encoder_train(encoder, [e_x_train_np, e_y_train_np], 100, 32, keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "call() missing 1 required positional argument: 'states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-7dfafab8e3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \"\"\"\n\u001b[0;32m-> 1051\u001b[0;31m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[1;32m   1052\u001b[0m                     signatures, options)\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    135\u001b[0m         model, filepath, overwrite, include_optimizer)\n\u001b[1;32m    136\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[0m\u001b[1;32m    138\u001b[0m                           signatures, options)\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# we use the default replica context here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0msave_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m    948\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m   _, exported_graph, object_saver, asset_info = _build_meta_graph(\n\u001b[0m\u001b[1;32m    951\u001b[0m       obj, export_dir, signatures, options, meta_graph_def)\n\u001b[1;32m    952\u001b[0m   \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_schema_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_SCHEMA_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, export_dir, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1005\u001b[0m   \u001b[0mcheckpoint_graph_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_AugmentedGraphView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msignatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m     signatures = signature_serialization.find_function_to_export(\n\u001b[0m\u001b[1;32m   1008\u001b[0m         checkpoint_graph_view)\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_serialization.py\u001b[0m in \u001b[0;36mfind_function_to_export\u001b[0;34m(saveable_view)\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;31m# If the user did not specify signatures, check the root object for a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;31m# that can be made into a signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m   \u001b[0mfunctions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_SIGNATURE_ATTR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msignature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36mlist_functions\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mobj_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj_functions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    143\u001b[0m           self._serialization_cache)\n\u001b[1;32m    144\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_list_functions_for_serialization\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m     functions = super(\n\u001b[0m\u001b[1;32m   1656\u001b[0m         Model, self)._list_functions_for_serialization(serialization_cache)\n\u001b[1;32m   1657\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_list_functions_for_serialization\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_list_functions_for_serialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2749\u001b[0;31m     return (self._trackable_saved_model_saver\n\u001b[0m\u001b[1;32m   2750\u001b[0m             .list_functions_for_serialization(serialization_cache))\n\u001b[1;32m   2751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\u001b[0m in \u001b[0;36mlist_functions_for_serialization\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mConcreteFunction\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mfns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions_to_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# The parent AutoTrackable class saves all user-defined tf.functions, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\u001b[0m in \u001b[0;36mfunctions_to_serialize\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfunctions_to_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     return (self._get_serialized_attributes(\n\u001b[0m\u001b[1;32m     77\u001b[0m         serialization_cache).functions_to_serialize)\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\u001b[0m in \u001b[0;36m_get_serialized_attributes\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mserialized_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     object_dict, function_dict = self._get_serialized_attributes_internal(\n\u001b[0m\u001b[1;32m     92\u001b[0m         serialization_cache)\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py\u001b[0m in \u001b[0;36m_get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# cache (i.e. this is the root level object).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialization_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKERAS_CACHE_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m       \u001b[0mdefault_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_save_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Other than the default signature function, all other attributes match with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\u001b[0m in \u001b[0;36mdefault_save_signature\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0moriginal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reset_layer_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_model_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m   \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0m_restore_layer_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mhas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0myet\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0mconcrete\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \"\"\"\n\u001b[0;32m--> 959\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36m_wrapped_model\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     with base_layer_utils.call_context().enter(\n\u001b[1;32m    131\u001b[0m         model, inputs=inputs, build_graph=False, training=False, saving=True):\n\u001b[0;32m--> 132\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Outputs always has to be a flat dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: call() missing 1 required positional argument: 'states'"
     ]
    }
   ],
   "source": [
    "encoder.save(\"yes\", save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions for LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
    "    ''' A training step, train a batch of the data and return the loss value reached\n",
    "        Input:\n",
    "        - input_seq: array of integers, shape [batch_size, seq_len, num_features].\n",
    "            the input sequence\n",
    "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, 1].\n",
    "            the target seq, our target sequence\n",
    "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, 1].\n",
    "            the input sequence to the decoder, we use Teacher Forcing\n",
    "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
    "            the initial state of the encoder\n",
    "        - optimizer: a tf.keras.optimizers.\n",
    "        Output:\n",
    "        - loss: loss value\n",
    "        \n",
    "    '''\n",
    "    # Networkâ€™s computations need to be put under tf.GradientTape() to keep track of gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the encoder outputs\n",
    "        en_outputs = encoder(input_seq, en_initial_states)\n",
    "        # Set the encoder and decoder states\n",
    "        en_states = en_outputs[1:]\n",
    "        de_states = en_states\n",
    "        # Get the encoder outputs\n",
    "        de_outputs = decoder(target_seq_in, de_states)\n",
    "        # Take the actual output\n",
    "        pred = de_outputs[0]\n",
    "        # Calculate the loss function\n",
    "        loss_func = keras.losses.MeanSquaredError()\n",
    "        loss = loss_func(target_seq_out, pred)\n",
    "\n",
    "    variables = decoder.trainable_variables\n",
    "    # Calculate the gradients for the variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # Apply the gradients and update the optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer):\n",
    "    losses = []\n",
    "\n",
    "    e_X = dataset[0]\n",
    "    e_y = dataset[1]\n",
    "    \n",
    "    d_X = dataset[2]\n",
    "    d_y = dataset[3]\n",
    "    for e in range(n_epochs):\n",
    "        # Get the initial time\n",
    "        start = time.time()\n",
    "        # Get the initial state for the encoder\n",
    "        en_initial_states = encoder.init_states(batch_size)\n",
    "        # Generate a random ordering for the data\n",
    "        perm = np.random.permutation(range(0, len(X)))\n",
    "        e_X = e_X[perm]\n",
    "        e_y = e_y[perm]\n",
    "        d_X = d_X[perm]\n",
    "        d_y = d_y[perm]\n",
    "        # For every batch data\n",
    "        for i in range(0, len(e_X), batch_size):\n",
    "            if i + batch_size >= len(e_X):\n",
    "                en_initial_states = encoder.init_states((len(e_X) - i))\n",
    "                input_seq = tf.constant(e_X[i:(len(e_X))])\n",
    "                target_seq_in = tf.constant(d_X[i:(len(e_X))])\n",
    "                target_out = tf.reshape(tf.constant(d_y[i:(len(e_X))]), ((len(e_X) - i), 1))\n",
    "            else:\n",
    "                input_seq = tf.constant(e_X[i:i+batch_size])\n",
    "                target_seq_in = tf.constant(d_X[i:i+batch_size])\n",
    "                target_out = tf.reshape(tf.constant(d_y[i:i+batch_size]), (batch_size, 1))\n",
    "                \n",
    "            # Train and get the loss value \n",
    "            loss = decoder_train_step(input_seq, target_seq_in, target_out, en_initial_states, optimizer)\n",
    "        \n",
    "            if i % 1000 == 0:\n",
    "                # Store the loss and accuracy values\n",
    "                losses.append(loss)\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(e + 1, i, loss.numpy()))\n",
    "    \n",
    "        print('Time taken for 1 epoch {:.4f} sec\\n'.format(time.time() - start))\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reserve validation set for each epoch?\n",
    "Set up hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ED_LSTM(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer ed_lstm_56 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1 Batch 0 Loss 41086.2891\n",
      "Epoch 1 Batch 4000 Loss 896945.9375\n",
      "Epoch 1 Batch 8000 Loss 720847.3125\n",
      "Epoch 1 Batch 12000 Loss 3697317.5000\n",
      "Time taken for 1 epoch 11.5324 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 29587.1758\n",
      "Epoch 2 Batch 4000 Loss 844177.3750\n",
      "Epoch 2 Batch 8000 Loss 678824.6250\n",
      "Epoch 2 Batch 12000 Loss 3614547.0000\n",
      "Time taken for 1 epoch 11.2010 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 22902.8320\n",
      "Epoch 3 Batch 4000 Loss 805505.6875\n",
      "Epoch 3 Batch 8000 Loss 644023.0000\n",
      "Epoch 3 Batch 12000 Loss 3529232.0000\n",
      "Time taken for 1 epoch 11.1959 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 17191.8145\n",
      "Epoch 4 Batch 4000 Loss 764954.2500\n",
      "Epoch 4 Batch 8000 Loss 606145.1250\n",
      "Epoch 4 Batch 12000 Loss 3447424.7500\n",
      "Time taken for 1 epoch 11.0071 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 12471.2832\n",
      "Epoch 5 Batch 4000 Loss 728387.0000\n",
      "Epoch 5 Batch 8000 Loss 574300.6250\n",
      "Epoch 5 Batch 12000 Loss 3369269.0000\n",
      "Time taken for 1 epoch 11.2692 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 8951.8613\n",
      "Epoch 6 Batch 4000 Loss 699277.2500\n",
      "Epoch 6 Batch 8000 Loss 543396.1875\n",
      "Epoch 6 Batch 12000 Loss 3295155.7500\n",
      "Time taken for 1 epoch 11.1413 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 6574.4443\n",
      "Epoch 7 Batch 4000 Loss 663635.3750\n",
      "Epoch 7 Batch 8000 Loss 516896.4688\n",
      "Epoch 7 Batch 12000 Loss 3245935.7500\n",
      "Time taken for 1 epoch 11.1343 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 4999.0293\n",
      "Epoch 8 Batch 4000 Loss 638837.8125\n",
      "Epoch 8 Batch 8000 Loss 491670.5312\n",
      "Epoch 8 Batch 12000 Loss 3183794.2500\n",
      "Time taken for 1 epoch 11.1115 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 4220.1006\n",
      "Epoch 9 Batch 4000 Loss 603290.4375\n",
      "Epoch 9 Batch 8000 Loss 463370.4375\n",
      "Epoch 9 Batch 12000 Loss 3101601.5000\n",
      "Time taken for 1 epoch 11.1590 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 4121.5293\n",
      "Epoch 10 Batch 4000 Loss 569931.8750\n",
      "Epoch 10 Batch 8000 Loss 431987.4688\n",
      "Epoch 10 Batch 12000 Loss 3020592.5000\n",
      "Time taken for 1 epoch 11.1151 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 4217.1445\n",
      "Epoch 11 Batch 4000 Loss 539669.5000\n",
      "Epoch 11 Batch 8000 Loss 402277.9375\n",
      "Epoch 11 Batch 12000 Loss 2930408.7500\n",
      "Time taken for 1 epoch 11.1655 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 4777.2817\n",
      "Epoch 12 Batch 4000 Loss 517061.9375\n",
      "Epoch 12 Batch 8000 Loss 386486.3125\n",
      "Epoch 12 Batch 12000 Loss 2895261.5000\n",
      "Time taken for 1 epoch 11.1495 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 5474.3916\n",
      "Epoch 13 Batch 4000 Loss 485559.0000\n",
      "Epoch 13 Batch 8000 Loss 356760.9062\n",
      "Epoch 13 Batch 12000 Loss 2834466.7500\n",
      "Time taken for 1 epoch 11.1332 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 5327.5869\n",
      "Epoch 14 Batch 4000 Loss 476569.0625\n",
      "Epoch 14 Batch 8000 Loss 341452.5625\n",
      "Epoch 14 Batch 12000 Loss 2766786.0000\n",
      "Time taken for 1 epoch 11.1411 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 5619.2847\n",
      "Epoch 15 Batch 4000 Loss 436100.0938\n",
      "Epoch 15 Batch 8000 Loss 313425.8438\n",
      "Epoch 15 Batch 12000 Loss 2670628.7500\n",
      "Time taken for 1 epoch 11.1656 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 4803.7783\n",
      "Epoch 16 Batch 4000 Loss 430490.6875\n",
      "Epoch 16 Batch 8000 Loss 297507.8750\n",
      "Epoch 16 Batch 12000 Loss 2621840.0000\n",
      "Time taken for 1 epoch 11.1535 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 5935.0034\n",
      "Epoch 17 Batch 4000 Loss 395842.3438\n",
      "Epoch 17 Batch 8000 Loss 277624.6250\n",
      "Epoch 17 Batch 12000 Loss 2568782.7500\n",
      "Time taken for 1 epoch 11.0383 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 5035.6143\n",
      "Epoch 18 Batch 4000 Loss 362156.9062\n",
      "Epoch 18 Batch 8000 Loss 256028.5312\n",
      "Epoch 18 Batch 12000 Loss 2517084.5000\n",
      "Time taken for 1 epoch 11.0876 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 5876.6611\n",
      "Epoch 19 Batch 4000 Loss 346195.4375\n",
      "Epoch 19 Batch 8000 Loss 242391.1094\n",
      "Epoch 19 Batch 12000 Loss 2472354.0000\n",
      "Time taken for 1 epoch 11.3530 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 5884.0825\n",
      "Epoch 20 Batch 4000 Loss 327294.0000\n",
      "Epoch 20 Batch 8000 Loss 223277.5156\n",
      "Epoch 20 Batch 12000 Loss 2387909.7500\n",
      "Time taken for 1 epoch 11.5536 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 5023.9575\n",
      "Epoch 21 Batch 4000 Loss 312985.7188\n",
      "Epoch 21 Batch 8000 Loss 207759.9219\n",
      "Epoch 21 Batch 12000 Loss 2339686.5000\n",
      "Time taken for 1 epoch 11.6664 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 5187.3105\n",
      "Epoch 22 Batch 4000 Loss 286429.6250\n",
      "Epoch 22 Batch 8000 Loss 192635.6094\n",
      "Epoch 22 Batch 12000 Loss 2291481.5000\n",
      "Time taken for 1 epoch 11.2698 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 5497.9580\n",
      "Epoch 23 Batch 4000 Loss 269090.1875\n",
      "Epoch 23 Batch 8000 Loss 181538.5156\n",
      "Epoch 23 Batch 12000 Loss 2243625.5000\n",
      "Time taken for 1 epoch 11.4575 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 5450.7485\n",
      "Epoch 24 Batch 4000 Loss 252855.7188\n",
      "Epoch 24 Batch 8000 Loss 175114.5000\n",
      "Epoch 24 Batch 12000 Loss 2174763.2500\n",
      "Time taken for 1 epoch 11.2651 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 4856.7144\n",
      "Epoch 25 Batch 4000 Loss 237442.0938\n",
      "Epoch 25 Batch 8000 Loss 153284.0469\n",
      "Epoch 25 Batch 12000 Loss 2118903.5000\n",
      "Time taken for 1 epoch 11.4686 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 4790.4810\n",
      "Epoch 26 Batch 4000 Loss 231351.3125\n",
      "Epoch 26 Batch 8000 Loss 141573.4062\n",
      "Epoch 26 Batch 12000 Loss 2067687.0000\n",
      "Time taken for 1 epoch 11.4276 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 5771.4253\n",
      "Epoch 27 Batch 4000 Loss 208804.7188\n",
      "Epoch 27 Batch 8000 Loss 137006.7656\n",
      "Epoch 27 Batch 12000 Loss 2032519.6250\n",
      "Time taken for 1 epoch 11.1816 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 5349.6064\n",
      "Epoch 28 Batch 4000 Loss 203322.2969\n",
      "Epoch 28 Batch 8000 Loss 120476.8984\n",
      "Epoch 28 Batch 12000 Loss 1970854.7500\n",
      "Time taken for 1 epoch 11.2108 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 4709.7363\n",
      "Epoch 29 Batch 4000 Loss 183101.7656\n",
      "Epoch 29 Batch 8000 Loss 110917.9141\n",
      "Epoch 29 Batch 12000 Loss 1922882.5000\n",
      "Time taken for 1 epoch 11.1691 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 4835.8447\n",
      "Epoch 30 Batch 4000 Loss 171334.2500\n",
      "Epoch 30 Batch 8000 Loss 108168.8984\n",
      "Epoch 30 Batch 12000 Loss 1908197.2500\n",
      "Time taken for 1 epoch 11.2523 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 4660.6865\n",
      "Epoch 31 Batch 4000 Loss 167876.1250\n",
      "Epoch 31 Batch 8000 Loss 94151.1875\n",
      "Epoch 31 Batch 12000 Loss 1832955.7500\n",
      "Time taken for 1 epoch 11.2515 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 5248.7407\n",
      "Epoch 32 Batch 4000 Loss 150147.2188\n",
      "Epoch 32 Batch 8000 Loss 86636.4609\n",
      "Epoch 32 Batch 12000 Loss 1787980.7500\n",
      "Time taken for 1 epoch 11.8340 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 5039.2412\n",
      "Epoch 33 Batch 4000 Loss 143830.2500\n",
      "Epoch 33 Batch 8000 Loss 81298.0938\n",
      "Epoch 33 Batch 12000 Loss 1759286.1250\n",
      "Time taken for 1 epoch 12.1136 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 5040.9551\n",
      "Epoch 34 Batch 4000 Loss 131537.6094\n",
      "Epoch 34 Batch 8000 Loss 78510.8984\n",
      "Epoch 34 Batch 12000 Loss 1737305.2500\n",
      "Time taken for 1 epoch 11.6071 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 4758.5713\n",
      "Epoch 35 Batch 4000 Loss 130088.0547\n",
      "Epoch 35 Batch 8000 Loss 72694.3281\n",
      "Epoch 35 Batch 12000 Loss 1699135.5000\n",
      "Time taken for 1 epoch 11.5266 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 5423.5654\n",
      "Epoch 36 Batch 4000 Loss 121752.0234\n",
      "Epoch 36 Batch 8000 Loss 65128.0273\n",
      "Epoch 36 Batch 12000 Loss 1641318.7500\n",
      "Time taken for 1 epoch 11.2850 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 5682.7324\n",
      "Epoch 37 Batch 4000 Loss 108845.4453\n",
      "Epoch 37 Batch 8000 Loss 62113.1797\n",
      "Epoch 37 Batch 12000 Loss 1653502.5000\n",
      "Time taken for 1 epoch 11.2462 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 5876.5635\n",
      "Epoch 38 Batch 4000 Loss 112632.9766\n",
      "Epoch 38 Batch 8000 Loss 60105.4727\n",
      "Epoch 38 Batch 12000 Loss 1603070.8750\n",
      "Time taken for 1 epoch 11.4456 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 5126.0610\n",
      "Epoch 39 Batch 4000 Loss 102739.6562\n",
      "Epoch 39 Batch 8000 Loss 54310.1094\n",
      "Epoch 39 Batch 12000 Loss 1547115.1250\n",
      "Time taken for 1 epoch 11.4008 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 5248.5732\n",
      "Epoch 40 Batch 4000 Loss 95525.6875\n",
      "Epoch 40 Batch 8000 Loss 54030.6094\n",
      "Epoch 40 Batch 12000 Loss 1537224.0000\n",
      "Time taken for 1 epoch 11.4269 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 5448.9004\n",
      "Epoch 41 Batch 4000 Loss 95055.7266\n",
      "Epoch 41 Batch 8000 Loss 49419.3477\n",
      "Epoch 41 Batch 12000 Loss 1496593.2500\n",
      "Time taken for 1 epoch 11.8855 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 5386.4551\n",
      "Epoch 42 Batch 4000 Loss 89811.1953\n",
      "Epoch 42 Batch 8000 Loss 46702.4609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 12000 Loss 1455177.1250\n",
      "Time taken for 1 epoch 11.5416 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 5638.2568\n",
      "Epoch 43 Batch 4000 Loss 83767.1875\n",
      "Epoch 43 Batch 8000 Loss 45590.4375\n",
      "Epoch 43 Batch 12000 Loss 1412919.2500\n",
      "Time taken for 1 epoch 11.5142 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 5494.7002\n",
      "Epoch 44 Batch 4000 Loss 79812.5156\n",
      "Epoch 44 Batch 8000 Loss 40980.5312\n",
      "Epoch 44 Batch 12000 Loss 1378690.5000\n",
      "Time taken for 1 epoch 11.6120 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 4883.4863\n",
      "Epoch 45 Batch 4000 Loss 76288.7500\n",
      "Epoch 45 Batch 8000 Loss 40379.4219\n",
      "Epoch 45 Batch 12000 Loss 1346587.5000\n",
      "Time taken for 1 epoch 11.3772 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 5375.7334\n",
      "Epoch 46 Batch 4000 Loss 73855.2344\n",
      "Epoch 46 Batch 8000 Loss 37750.3594\n",
      "Epoch 46 Batch 12000 Loss 1312668.6250\n",
      "Time taken for 1 epoch 11.4230 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 6774.6445\n",
      "Epoch 47 Batch 4000 Loss 71290.6406\n",
      "Epoch 47 Batch 8000 Loss 38003.9219\n",
      "Epoch 47 Batch 12000 Loss 1317147.2500\n",
      "Time taken for 1 epoch 11.5041 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 5841.5205\n",
      "Epoch 48 Batch 4000 Loss 68145.6875\n",
      "Epoch 48 Batch 8000 Loss 38266.0742\n",
      "Epoch 48 Batch 12000 Loss 1272175.8750\n",
      "Time taken for 1 epoch 11.4486 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 6158.9043\n",
      "Epoch 49 Batch 4000 Loss 68394.0625\n",
      "Epoch 49 Batch 8000 Loss 34937.5625\n",
      "Epoch 49 Batch 12000 Loss 1234436.7500\n",
      "Time taken for 1 epoch 11.4786 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 5823.0176\n",
      "Epoch 50 Batch 4000 Loss 66900.3516\n",
      "Epoch 50 Batch 8000 Loss 37250.6250\n",
      "Epoch 50 Batch 12000 Loss 1213715.2500\n",
      "Time taken for 1 epoch 11.4542 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 6771.3413\n",
      "Epoch 51 Batch 4000 Loss 65575.9609\n",
      "Epoch 51 Batch 8000 Loss 31111.9395\n",
      "Epoch 51 Batch 12000 Loss 1181138.0000\n",
      "Time taken for 1 epoch 11.5074 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 8423.3164\n",
      "Epoch 52 Batch 4000 Loss 66708.7500\n",
      "Epoch 52 Batch 8000 Loss 34159.2500\n",
      "Epoch 52 Batch 12000 Loss 1131400.7500\n",
      "Time taken for 1 epoch 11.2885 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 6569.4521\n",
      "Epoch 53 Batch 4000 Loss 65179.9531\n",
      "Epoch 53 Batch 8000 Loss 28402.6309\n",
      "Epoch 53 Batch 12000 Loss 1102574.0000\n",
      "Time taken for 1 epoch 11.2888 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 5842.6475\n",
      "Epoch 54 Batch 4000 Loss 67008.1328\n",
      "Epoch 54 Batch 8000 Loss 27641.8984\n",
      "Epoch 54 Batch 12000 Loss 1067329.2500\n",
      "Time taken for 1 epoch 11.3044 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 6772.6172\n",
      "Epoch 55 Batch 4000 Loss 68327.4375\n",
      "Epoch 55 Batch 8000 Loss 33534.7031\n",
      "Epoch 55 Batch 12000 Loss 1058178.7500\n",
      "Time taken for 1 epoch 11.2937 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 5224.8184\n",
      "Epoch 56 Batch 4000 Loss 64483.1836\n",
      "Epoch 56 Batch 8000 Loss 36741.0234\n",
      "Epoch 56 Batch 12000 Loss 1007029.6250\n",
      "Time taken for 1 epoch 11.4697 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 6515.3774\n",
      "Epoch 57 Batch 4000 Loss 62892.4062\n",
      "Epoch 57 Batch 8000 Loss 33947.8438\n",
      "Epoch 57 Batch 12000 Loss 967869.3750\n",
      "Time taken for 1 epoch 11.6292 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 5481.9331\n",
      "Epoch 58 Batch 4000 Loss 65216.0391\n",
      "Epoch 58 Batch 8000 Loss 31378.9297\n",
      "Epoch 58 Batch 12000 Loss 944163.0000\n",
      "Time taken for 1 epoch 11.2695 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 6280.5981\n",
      "Epoch 59 Batch 4000 Loss 69673.1953\n",
      "Epoch 59 Batch 8000 Loss 44150.6562\n",
      "Epoch 59 Batch 12000 Loss 913579.9375\n",
      "Time taken for 1 epoch 11.0998 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 4675.0439\n",
      "Epoch 60 Batch 4000 Loss 60595.2500\n",
      "Epoch 60 Batch 8000 Loss 35721.5859\n",
      "Epoch 60 Batch 12000 Loss 940793.0625\n",
      "Time taken for 1 epoch 11.1574 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 6164.4575\n",
      "Epoch 61 Batch 4000 Loss 61937.3438\n",
      "Epoch 61 Batch 8000 Loss 33648.5859\n",
      "Epoch 61 Batch 12000 Loss 951406.6875\n",
      "Time taken for 1 epoch 11.2199 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 6058.5723\n",
      "Epoch 62 Batch 4000 Loss 63431.3086\n",
      "Epoch 62 Batch 8000 Loss 31379.3125\n",
      "Epoch 62 Batch 12000 Loss 893546.0000\n",
      "Time taken for 1 epoch 11.1764 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 5828.6216\n",
      "Epoch 63 Batch 4000 Loss 64602.5508\n",
      "Epoch 63 Batch 8000 Loss 39489.2344\n",
      "Epoch 63 Batch 12000 Loss 838343.6250\n",
      "Time taken for 1 epoch 11.1713 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 5750.3389\n",
      "Epoch 64 Batch 4000 Loss 64316.8008\n",
      "Epoch 64 Batch 8000 Loss 42605.8008\n",
      "Epoch 64 Batch 12000 Loss 865926.1250\n",
      "Time taken for 1 epoch 11.2965 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 5623.0972\n",
      "Epoch 65 Batch 4000 Loss 61422.9375\n",
      "Epoch 65 Batch 8000 Loss 35536.5391\n",
      "Epoch 65 Batch 12000 Loss 843600.5625\n",
      "Time taken for 1 epoch 11.2446 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 8990.5117\n",
      "Epoch 66 Batch 4000 Loss 67870.3516\n",
      "Epoch 66 Batch 8000 Loss 43794.2656\n",
      "Epoch 66 Batch 12000 Loss 815155.3750\n",
      "Time taken for 1 epoch 11.2107 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 6899.9805\n",
      "Epoch 67 Batch 4000 Loss 64713.2148\n",
      "Epoch 67 Batch 8000 Loss 31070.7422\n",
      "Epoch 67 Batch 12000 Loss 786316.1250\n",
      "Time taken for 1 epoch 11.2768 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 6630.0898\n",
      "Epoch 68 Batch 4000 Loss 69620.6484\n",
      "Epoch 68 Batch 8000 Loss 50486.2773\n",
      "Epoch 68 Batch 12000 Loss 761727.5000\n",
      "Time taken for 1 epoch 11.2168 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 6424.5015\n",
      "Epoch 69 Batch 4000 Loss 66474.8359\n",
      "Epoch 69 Batch 8000 Loss 42712.7930\n",
      "Epoch 69 Batch 12000 Loss 722331.5000\n",
      "Time taken for 1 epoch 11.1892 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 8416.5693\n",
      "Epoch 70 Batch 4000 Loss 77658.1250\n",
      "Epoch 70 Batch 8000 Loss 40042.3555\n",
      "Epoch 70 Batch 12000 Loss 706605.0000\n",
      "Time taken for 1 epoch 11.5720 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 7437.4961\n",
      "Epoch 71 Batch 4000 Loss 70013.1172\n",
      "Epoch 71 Batch 8000 Loss 43985.3086\n",
      "Epoch 71 Batch 12000 Loss 687068.5625\n",
      "Time taken for 1 epoch 11.2556 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 7817.4360\n",
      "Epoch 72 Batch 4000 Loss 78690.5859\n",
      "Epoch 72 Batch 8000 Loss 39576.4414\n",
      "Epoch 72 Batch 12000 Loss 668039.3125\n",
      "Time taken for 1 epoch 11.2372 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 6701.3584\n",
      "Epoch 73 Batch 4000 Loss 61271.2305\n",
      "Epoch 73 Batch 8000 Loss 40974.4219\n",
      "Epoch 73 Batch 12000 Loss 666427.0000\n",
      "Time taken for 1 epoch 11.2435 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 7217.6304\n",
      "Epoch 74 Batch 4000 Loss 82103.6953\n",
      "Epoch 74 Batch 8000 Loss 46997.5391\n",
      "Epoch 74 Batch 12000 Loss 680148.4375\n",
      "Time taken for 1 epoch 11.1911 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 5872.5615\n",
      "Epoch 75 Batch 4000 Loss 85179.6719\n",
      "Epoch 75 Batch 8000 Loss 36651.2109\n",
      "Epoch 75 Batch 12000 Loss 651512.5000\n",
      "Time taken for 1 epoch 11.2868 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 7272.5132\n",
      "Epoch 76 Batch 4000 Loss 73763.6562\n",
      "Epoch 76 Batch 8000 Loss 42625.7070\n",
      "Epoch 76 Batch 12000 Loss 612118.6875\n",
      "Time taken for 1 epoch 11.2465 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 6852.8936\n",
      "Epoch 77 Batch 4000 Loss 94220.8906\n",
      "Epoch 77 Batch 8000 Loss 31336.2988\n",
      "Epoch 77 Batch 12000 Loss 607729.2500\n",
      "Time taken for 1 epoch 11.6666 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 6295.4111\n",
      "Epoch 78 Batch 4000 Loss 72435.3906\n",
      "Epoch 78 Batch 8000 Loss 31313.6016\n",
      "Epoch 78 Batch 12000 Loss 606607.8750\n",
      "Time taken for 1 epoch 13.6179 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 7081.0225\n",
      "Epoch 79 Batch 4000 Loss 66363.8438\n",
      "Epoch 79 Batch 8000 Loss 35866.8516\n",
      "Epoch 79 Batch 12000 Loss 612584.2500\n",
      "Time taken for 1 epoch 13.9888 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 7404.2661\n",
      "Epoch 80 Batch 4000 Loss 59053.9648\n",
      "Epoch 80 Batch 8000 Loss 34865.6016\n",
      "Epoch 80 Batch 12000 Loss 591187.8750\n",
      "Time taken for 1 epoch 14.1050 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 6936.1738\n",
      "Epoch 81 Batch 4000 Loss 73214.3750\n",
      "Epoch 81 Batch 8000 Loss 36806.4766\n",
      "Epoch 81 Batch 12000 Loss 575480.2500\n",
      "Time taken for 1 epoch 14.2941 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 7029.5947\n",
      "Epoch 82 Batch 4000 Loss 78123.4766\n",
      "Epoch 82 Batch 8000 Loss 39936.9766\n",
      "Epoch 82 Batch 12000 Loss 559449.1875\n",
      "Time taken for 1 epoch 14.1376 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 7936.2690\n",
      "Epoch 83 Batch 4000 Loss 90858.9531\n",
      "Epoch 83 Batch 8000 Loss 42584.6953\n",
      "Epoch 83 Batch 12000 Loss 544671.0625\n",
      "Time taken for 1 epoch 14.3214 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 7012.1479\n",
      "Epoch 84 Batch 4000 Loss 89886.9141\n",
      "Epoch 84 Batch 8000 Loss 36395.6523\n",
      "Epoch 84 Batch 12000 Loss 545945.6250\n",
      "Time taken for 1 epoch 14.3032 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 6873.5366\n",
      "Epoch 85 Batch 4000 Loss 59074.5664\n",
      "Epoch 85 Batch 8000 Loss 49630.8984\n",
      "Epoch 85 Batch 12000 Loss 534031.2500\n",
      "Time taken for 1 epoch 13.7064 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 7264.9575\n",
      "Epoch 86 Batch 4000 Loss 58725.2539\n",
      "Epoch 86 Batch 8000 Loss 44235.3320\n",
      "Epoch 86 Batch 12000 Loss 532358.1250\n",
      "Time taken for 1 epoch 12.7597 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 7989.4619\n",
      "Epoch 87 Batch 4000 Loss 86561.0469\n",
      "Epoch 87 Batch 8000 Loss 68462.9219\n",
      "Epoch 87 Batch 12000 Loss 518236.7188\n",
      "Time taken for 1 epoch 12.6941 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 6295.9370\n",
      "Epoch 88 Batch 4000 Loss 97194.5781\n",
      "Epoch 88 Batch 8000 Loss 39616.5938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 12000 Loss 464815.8125\n",
      "Time taken for 1 epoch 12.7274 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 5716.0410\n",
      "Epoch 89 Batch 4000 Loss 80986.2188\n",
      "Epoch 89 Batch 8000 Loss 48320.6797\n",
      "Epoch 89 Batch 12000 Loss 450394.0000\n",
      "Time taken for 1 epoch 12.7120 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 7796.1201\n",
      "Epoch 90 Batch 4000 Loss 69667.2500\n",
      "Epoch 90 Batch 8000 Loss 44426.3594\n",
      "Epoch 90 Batch 12000 Loss 469840.1562\n",
      "Time taken for 1 epoch 12.6733 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 7955.9619\n",
      "Epoch 91 Batch 4000 Loss 75306.1562\n",
      "Epoch 91 Batch 8000 Loss 51594.2031\n",
      "Epoch 91 Batch 12000 Loss 481321.8125\n",
      "Time taken for 1 epoch 12.9408 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 6536.4346\n",
      "Epoch 92 Batch 4000 Loss 72647.5938\n",
      "Epoch 92 Batch 8000 Loss 37768.4375\n",
      "Epoch 92 Batch 12000 Loss 465475.0938\n",
      "Time taken for 1 epoch 13.0693 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 7044.1738\n",
      "Epoch 93 Batch 4000 Loss 79820.4688\n",
      "Epoch 93 Batch 8000 Loss 29455.7891\n",
      "Epoch 93 Batch 12000 Loss 444454.5000\n",
      "Time taken for 1 epoch 12.8365 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 6629.4346\n",
      "Epoch 94 Batch 4000 Loss 109197.7344\n",
      "Epoch 94 Batch 8000 Loss 34482.9961\n",
      "Epoch 94 Batch 12000 Loss 433246.9062\n",
      "Time taken for 1 epoch 12.8376 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 4755.7324\n",
      "Epoch 95 Batch 4000 Loss 61501.4844\n",
      "Epoch 95 Batch 8000 Loss 31845.7461\n",
      "Epoch 95 Batch 12000 Loss 432919.8750\n",
      "Time taken for 1 epoch 12.7193 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 5394.0186\n",
      "Epoch 96 Batch 4000 Loss 75024.3750\n",
      "Epoch 96 Batch 8000 Loss 46866.2969\n",
      "Epoch 96 Batch 12000 Loss 395777.6562\n",
      "Time taken for 1 epoch 13.1994 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 5194.1006\n",
      "Epoch 97 Batch 4000 Loss 81744.8438\n",
      "Epoch 97 Batch 8000 Loss 25984.7031\n",
      "Epoch 97 Batch 12000 Loss 396095.1875\n",
      "Time taken for 1 epoch 13.5052 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 5893.4268\n",
      "Epoch 98 Batch 4000 Loss 125940.6797\n",
      "Epoch 98 Batch 8000 Loss 33512.4219\n",
      "Epoch 98 Batch 12000 Loss 382664.6875\n",
      "Time taken for 1 epoch 13.3282 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 8011.6572\n",
      "Epoch 99 Batch 4000 Loss 68002.9766\n",
      "Epoch 99 Batch 8000 Loss 39381.2070\n",
      "Epoch 99 Batch 12000 Loss 368172.9375\n",
      "Time taken for 1 epoch 12.8827 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 7026.5830\n",
      "Epoch 100 Batch 4000 Loss 101465.8984\n",
      "Epoch 100 Batch 8000 Loss 32051.0625\n",
      "Epoch 100 Batch 12000 Loss 364494.8125\n",
      "Time taken for 1 epoch 12.7100 sec\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-f7557abcc6d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me_x_train_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_y_train_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_x_train_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_y_train_np\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-217-ebbd70520d47>\u001b[0m in \u001b[0;36mdecoder_train\u001b[0;34m(encoder, decoder, dataset, n_epochs, batch_size, optimizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time taken for 1 epoch {:.4f} sec\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "d_losses = decoder_train(encoder, decoder, [e_x_train_np, e_y_train_np, d_x_train_np, d_y_train_np], 100, 32, keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Counterfactual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seq, strictnesses, starting_case_num, encoder, decoder):\n",
    "    # Set the encoder initial state\n",
    "    e_initial_states = encoder.init_states(1)\n",
    "    e_outputs = encoder(tf.constant(np.array([input_seq])), e_initial_states)\n",
    "    # Set the decoder states to the encoder vector or encoder hidden state\n",
    "    \n",
    "    predictions = []\n",
    "    last_case_num = starting_case_num\n",
    "    for s in strictnesses:\n",
    "        d_input = np.array([[[s, last_case_num]]], dtype=object).astype('float32')\n",
    "        print(d_input)\n",
    "        # Decode and get the predicted daily case count\n",
    "        d_output = decoder(tf.constant(d_input), e_outputs[1:])\n",
    "        \n",
    "        predictions.append(d_output[0][0][0].numpy())\n",
    "        last_case_num = d_output[0]\n",
    "        \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1. 14.]]]\n",
      "[[[ 1.      98.88573]]]\n",
      "[[[  1.      143.94125]]]\n",
      "[[[  1.      180.32693]]]\n",
      "[[[  1.      202.14551]]]\n",
      "[[[  1.      227.19452]]]\n",
      "[[[  2.      266.78452]]]\n",
      "[[[  2.      309.24863]]]\n",
      "[[[  2.     344.2394]]]\n",
      "[[[  2.      361.55493]]]\n",
      "[[[  3.     372.8328]]]\n",
      "[[[  3.      373.10733]]]\n",
      "[[[  3.      373.35562]]]\n",
      "[[[  3.     373.5793]]]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(e_x_train_np[1207], train_df[1268:(1268+14)]['c1_school_closing'].to_numpy(), train_df['yesterdays_active_cases'].iloc[1268], encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0. 14.]]]\n",
      "[[[  0.      100.99492]]]\n",
      "[[[  0.      131.49692]]]\n",
      "[[[  0.      167.70425]]]\n",
      "[[[  0.      191.46277]]]\n",
      "[[[  0.      214.11478]]]\n",
      "[[[  0.     248.7244]]]\n",
      "[[[  0.      310.96475]]]\n",
      "[[[  0.      360.00494]]]\n",
      "[[[  0.      397.42532]]]\n",
      "[[[  0.      420.66766]]]\n",
      "[[[  0.     440.6528]]]\n",
      "[[[  0.      451.90363]]]\n",
      "[[[  0.      456.55853]]]\n",
      "[[[  0.      458.30386]]]\n",
      "[[[  0.     458.9406]]]\n",
      "[[[  0.      459.17102]]]\n",
      "[[[  0.      459.25412]]]\n",
      "[[[  0.     459.2841]]]\n",
      "[[[  0.      459.29492]]]\n",
      "[[[  0.      459.29886]]]\n",
      "[[[  0.      459.30023]]]\n",
      "[[[  0.      459.30075]]]\n",
      "[[[  0.      459.30093]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n",
      "[[[  0.    459.301]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[100.99492,\n",
       " 131.49692,\n",
       " 167.70425,\n",
       " 191.46277,\n",
       " 214.11478,\n",
       " 248.7244,\n",
       " 310.96475,\n",
       " 360.00494,\n",
       " 397.42532,\n",
       " 420.66766,\n",
       " 440.6528,\n",
       " 451.90363,\n",
       " 456.55853,\n",
       " 458.30386,\n",
       " 458.9406,\n",
       " 459.17102,\n",
       " 459.25412,\n",
       " 459.2841,\n",
       " 459.29492,\n",
       " 459.29886,\n",
       " 459.30023,\n",
       " 459.30075,\n",
       " 459.30093,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301,\n",
       " 459.301]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(e_x_train_np[1207], np.array([0]*50), train_df['yesterdays_active_cases'].iloc[1268], encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_df[1268:(1268+14)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98.88573,\n",
       " 143.94125,\n",
       " 180.32693,\n",
       " 202.14551,\n",
       " 227.19452,\n",
       " 266.78452,\n",
       " 309.24863,\n",
       " 344.2394,\n",
       " 361.55493,\n",
       " 372.8328,\n",
       " 373.10733,\n",
       " 373.35562,\n",
       " 373.5793,\n",
       " 373.78]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f14f013d190>]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_df.loc[(train_df.state == 'California') & \n",
    "             (train_df.date >= pd.to_datetime(\"2020-02-29\")) & \n",
    "             (train_df.date < pd.to_datetime(\"2020-03-21\")) &\n",
    "             (train_df.date.dt.year == 2020)]['cases'].to_numpy())\n",
    "plt.plot(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-02-29 00:00:00')"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(\"2020-02-29\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>c1_school_closing</th>\n",
       "      <th>yesterdays_active_cases</th>\n",
       "      <th>pop_density</th>\n",
       "      <th>scaled_median_income</th>\n",
       "      <th>political_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>241.377092</td>\n",
       "      <td>0.715723</td>\n",
       "      <td>0.301093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state       date  cases  deaths  c1_school_closing  \\\n",
       "1251  California 2020-02-19    0.0     0.0                0.0   \n",
       "1252  California 2020-02-20    1.0     0.0                0.0   \n",
       "1253  California 2020-02-21    1.0     0.0                0.0   \n",
       "1254  California 2020-02-22    0.0     0.0                0.0   \n",
       "1255  California 2020-02-23    0.0     0.0                0.0   \n",
       "1256  California 2020-02-24    2.0     0.0                0.0   \n",
       "1257  California 2020-02-25    0.0     0.0                0.0   \n",
       "1258  California 2020-02-26   15.0     0.0                0.0   \n",
       "1259  California 2020-02-27    0.0     0.0                0.0   \n",
       "1260  California 2020-02-28    1.0     0.0                0.0   \n",
       "1261  California 2020-02-29    1.0     0.0                0.0   \n",
       "1262  California 2020-03-01    5.0     0.0                0.0   \n",
       "1263  California 2020-03-02    5.0     0.0                0.0   \n",
       "1264  California 2020-03-03    7.0     0.0                0.0   \n",
       "1265  California 2020-03-04   10.0     1.0                0.0   \n",
       "1266  California 2020-03-05   12.0     0.0                0.0   \n",
       "1267  California 2020-03-06   14.0     0.0                0.0   \n",
       "1268  California 2020-03-07   19.0     0.0                1.0   \n",
       "1269  California 2020-03-08   12.0     0.0                1.0   \n",
       "1270  California 2020-03-09   60.0     1.0                1.0   \n",
       "1271  California 2020-03-10    7.0     1.0                1.0   \n",
       "1272  California 2020-03-11   23.0     1.0                1.0   \n",
       "1273  California 2020-03-12   50.0     0.0                1.0   \n",
       "1274  California 2020-03-13   68.0     1.0                2.0   \n",
       "1275  California 2020-03-14   61.0     0.0                2.0   \n",
       "\n",
       "      yesterdays_active_cases  pop_density  scaled_median_income  \\\n",
       "1251                      0.0   241.377092              0.715723   \n",
       "1252                      0.0   241.377092              0.715723   \n",
       "1253                      1.0   241.377092              0.715723   \n",
       "1254                      1.0   241.377092              0.715723   \n",
       "1255                      0.0   241.377092              0.715723   \n",
       "1256                      0.0   241.377092              0.715723   \n",
       "1257                      2.0   241.377092              0.715723   \n",
       "1258                      0.0   241.377092              0.715723   \n",
       "1259                     15.0   241.377092              0.715723   \n",
       "1260                      0.0   241.377092              0.715723   \n",
       "1261                      1.0   241.377092              0.715723   \n",
       "1262                      1.0   241.377092              0.715723   \n",
       "1263                      5.0   241.377092              0.715723   \n",
       "1264                      5.0   241.377092              0.715723   \n",
       "1265                      7.0   241.377092              0.715723   \n",
       "1266                     10.0   241.377092              0.715723   \n",
       "1267                     12.0   241.377092              0.715723   \n",
       "1268                     14.0   241.377092              0.715723   \n",
       "1269                     19.0   241.377092              0.715723   \n",
       "1270                     12.0   241.377092              0.715723   \n",
       "1271                     60.0   241.377092              0.715723   \n",
       "1272                      7.0   241.377092              0.715723   \n",
       "1273                     23.0   241.377092              0.715723   \n",
       "1274                     50.0   241.377092              0.715723   \n",
       "1275                     68.0   241.377092              0.715723   \n",
       "\n",
       "      political_index  \n",
       "1251         0.301093  \n",
       "1252         0.301093  \n",
       "1253         0.301093  \n",
       "1254         0.301093  \n",
       "1255         0.301093  \n",
       "1256         0.301093  \n",
       "1257         0.301093  \n",
       "1258         0.301093  \n",
       "1259         0.301093  \n",
       "1260         0.301093  \n",
       "1261         0.301093  \n",
       "1262         0.301093  \n",
       "1263         0.301093  \n",
       "1264         0.301093  \n",
       "1265         0.301093  \n",
       "1266         0.301093  \n",
       "1267         0.301093  \n",
       "1268         0.301093  \n",
       "1269         0.301093  \n",
       "1270         0.301093  \n",
       "1271         0.301093  \n",
       "1272         0.301093  \n",
       "1273         0.301093  \n",
       "1274         0.301093  \n",
       "1275         0.301093  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df['state'] == 'California'][25:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['yesterdays_active_cases'].iloc[1268]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   1.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,   1.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,   5.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,   5.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,   7.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,  10.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,  12.        , 241.37709154,   0.71572337,\n",
       "          0.30109293]])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_x_train_np[1205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[1268:(1268+14)]['c1_school_closing'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170\n",
      "0.3010929306611844\n",
      "1171\n",
      "0.3010929306611844\n",
      "1172\n",
      "0.3010929306611844\n",
      "1173\n",
      "0.3010929306611844\n",
      "1174\n",
      "0.3010929306611844\n",
      "1175\n",
      "0.3010929306611844\n",
      "1176\n",
      "0.3010929306611844\n",
      "1177\n",
      "0.3010929306611844\n",
      "1178\n",
      "0.3010929306611844\n",
      "1179\n",
      "0.3010929306611844\n",
      "1180\n",
      "0.3010929306611844\n",
      "1181\n",
      "0.3010929306611844\n",
      "1182\n",
      "0.3010929306611844\n",
      "1183\n",
      "0.3010929306611844\n",
      "1184\n",
      "0.3010929306611844\n",
      "1185\n",
      "0.3010929306611844\n",
      "1186\n",
      "0.3010929306611844\n",
      "1187\n",
      "0.3010929306611844\n",
      "1188\n",
      "0.3010929306611844\n",
      "1189\n",
      "0.3010929306611844\n",
      "1190\n",
      "0.3010929306611844\n",
      "1191\n",
      "0.3010929306611844\n",
      "1192\n",
      "0.3010929306611844\n",
      "1193\n",
      "0.3010929306611844\n",
      "1194\n",
      "0.3010929306611844\n",
      "1195\n",
      "0.3010929306611844\n",
      "1196\n",
      "0.3010929306611844\n",
      "1197\n",
      "0.3010929306611844\n",
      "1198\n",
      "0.3010929306611844\n",
      "1199\n",
      "0.3010929306611844\n",
      "1200\n",
      "0.3010929306611844\n",
      "1201\n",
      "0.3010929306611844\n",
      "1202\n",
      "0.3010929306611844\n",
      "1203\n",
      "0.3010929306611844\n",
      "1204\n",
      "0.3010929306611844\n",
      "1205\n",
      "0.3010929306611844\n",
      "1206\n",
      "0.3010929306611844\n",
      "1207\n",
      "0.3010929306611844\n",
      "1208\n",
      "0.3010929306611844\n",
      "1209\n",
      "0.3010929306611844\n",
      "1210\n",
      "0.3010929306611844\n",
      "1211\n",
      "0.3010929306611844\n",
      "1212\n",
      "0.3010929306611844\n",
      "1213\n",
      "0.3010929306611844\n",
      "1214\n",
      "0.3010929306611844\n",
      "1215\n",
      "0.3010929306611844\n",
      "1216\n",
      "0.3010929306611844\n",
      "1217\n",
      "0.3010929306611844\n",
      "1218\n",
      "0.3010929306611844\n",
      "1219\n",
      "0.3010929306611844\n",
      "1220\n",
      "0.3010929306611844\n",
      "1221\n",
      "0.3010929306611844\n",
      "1222\n",
      "0.3010929306611844\n",
      "1223\n",
      "0.3010929306611844\n",
      "1224\n",
      "0.3010929306611844\n",
      "1225\n",
      "0.3010929306611844\n",
      "1226\n",
      "0.3010929306611844\n",
      "1227\n",
      "0.3010929306611844\n",
      "1228\n",
      "0.3010929306611844\n",
      "1229\n",
      "0.3010929306611844\n",
      "1230\n",
      "0.3010929306611844\n",
      "1231\n",
      "0.3010929306611844\n",
      "1232\n",
      "0.3010929306611844\n",
      "1233\n",
      "0.3010929306611844\n",
      "1234\n",
      "0.3010929306611844\n",
      "1235\n",
      "0.3010929306611844\n",
      "1236\n",
      "0.3010929306611844\n",
      "1237\n",
      "0.3010929306611844\n",
      "1238\n",
      "0.3010929306611844\n",
      "1239\n",
      "0.3010929306611844\n",
      "1240\n",
      "0.3010929306611844\n",
      "1241\n",
      "0.3010929306611844\n",
      "1242\n",
      "0.3010929306611844\n",
      "1243\n",
      "0.3010929306611844\n",
      "1244\n",
      "0.3010929306611844\n",
      "1245\n",
      "0.3010929306611844\n",
      "1246\n",
      "0.3010929306611844\n",
      "1247\n",
      "0.3010929306611844\n",
      "1248\n",
      "0.3010929306611844\n",
      "1249\n",
      "0.3010929306611844\n",
      "1250\n",
      "0.3010929306611844\n",
      "1251\n",
      "0.3010929306611844\n",
      "1252\n",
      "0.3010929306611844\n",
      "1253\n",
      "0.3010929306611844\n",
      "1254\n",
      "0.3010929306611844\n",
      "1255\n",
      "0.3010929306611844\n",
      "1256\n",
      "0.3010929306611844\n",
      "1257\n",
      "0.3010929306611844\n",
      "1258\n",
      "0.3010929306611844\n",
      "1259\n",
      "0.3010929306611844\n",
      "1260\n",
      "0.3010929306611844\n",
      "1261\n",
      "0.3010929306611844\n",
      "1262\n",
      "0.3010929306611844\n",
      "1263\n",
      "0.3010929306611844\n",
      "1264\n",
      "0.3010929306611844\n",
      "1265\n",
      "0.3010929306611844\n",
      "1266\n",
      "0.3010929306611844\n",
      "1267\n",
      "0.3010929306611844\n",
      "1268\n",
      "0.3010929306611844\n",
      "1269\n",
      "0.3010929306611844\n",
      "1270\n",
      "0.3010929306611844\n",
      "1271\n",
      "0.3010929306611844\n",
      "1272\n",
      "0.3010929306611844\n",
      "1273\n",
      "0.3010929306611844\n",
      "1274\n",
      "0.3010929306611844\n",
      "1275\n",
      "0.3010929306611844\n",
      "1276\n",
      "0.3010929306611844\n",
      "1277\n",
      "0.3010929306611844\n",
      "1278\n",
      "0.3010929306611844\n",
      "1279\n",
      "0.3010929306611844\n",
      "1280\n",
      "0.3010929306611844\n",
      "1281\n",
      "0.3010929306611844\n",
      "1282\n",
      "0.3010929306611844\n",
      "1283\n",
      "0.3010929306611844\n",
      "1284\n",
      "0.3010929306611844\n",
      "1285\n",
      "0.3010929306611844\n",
      "1286\n",
      "0.3010929306611844\n",
      "1287\n",
      "0.3010929306611844\n",
      "1288\n",
      "0.3010929306611844\n",
      "1289\n",
      "0.3010929306611844\n",
      "1290\n",
      "0.3010929306611844\n",
      "1291\n",
      "0.3010929306611844\n",
      "1292\n",
      "0.3010929306611844\n",
      "1293\n",
      "0.3010929306611844\n",
      "1294\n",
      "0.3010929306611844\n",
      "1295\n",
      "0.3010929306611844\n",
      "1296\n",
      "0.3010929306611844\n",
      "1297\n",
      "0.3010929306611844\n",
      "1298\n",
      "0.3010929306611844\n",
      "1299\n",
      "0.3010929306611844\n",
      "1300\n",
      "0.3010929306611844\n",
      "1301\n",
      "0.3010929306611844\n",
      "1302\n",
      "0.3010929306611844\n",
      "1303\n",
      "0.3010929306611844\n",
      "1304\n",
      "0.3010929306611844\n",
      "1305\n",
      "0.3010929306611844\n",
      "1306\n",
      "0.3010929306611844\n",
      "1307\n",
      "0.3010929306611844\n",
      "1308\n",
      "0.3010929306611844\n",
      "1309\n",
      "0.3010929306611844\n",
      "1310\n",
      "0.3010929306611844\n",
      "1311\n",
      "0.3010929306611844\n",
      "1312\n",
      "0.3010929306611844\n",
      "1313\n",
      "0.3010929306611844\n",
      "1314\n",
      "0.3010929306611844\n",
      "1315\n",
      "0.3010929306611844\n",
      "1316\n",
      "0.3010929306611844\n",
      "1317\n",
      "0.3010929306611844\n",
      "1318\n",
      "0.3010929306611844\n",
      "1319\n",
      "0.3010929306611844\n",
      "1320\n",
      "0.3010929306611844\n",
      "1321\n",
      "0.3010929306611844\n",
      "1322\n",
      "0.3010929306611844\n",
      "1323\n",
      "0.3010929306611844\n",
      "1324\n",
      "0.3010929306611844\n",
      "1325\n",
      "0.3010929306611844\n",
      "1326\n",
      "0.3010929306611844\n",
      "1327\n",
      "0.3010929306611844\n",
      "1328\n",
      "0.3010929306611844\n",
      "1329\n",
      "0.3010929306611844\n",
      "1330\n",
      "0.3010929306611844\n",
      "1331\n",
      "0.3010929306611844\n",
      "1332\n",
      "0.3010929306611844\n",
      "1333\n",
      "0.3010929306611844\n",
      "1334\n",
      "0.3010929306611844\n",
      "1335\n",
      "0.3010929306611844\n",
      "1336\n",
      "0.3010929306611844\n",
      "1337\n",
      "0.3010929306611844\n",
      "1338\n",
      "0.3010929306611844\n",
      "1339\n",
      "0.3010929306611844\n",
      "1340\n",
      "0.3010929306611844\n",
      "1341\n",
      "0.3010929306611844\n",
      "1342\n",
      "0.3010929306611844\n",
      "1343\n",
      "0.3010929306611844\n",
      "1344\n",
      "0.3010929306611844\n",
      "1345\n",
      "0.3010929306611844\n",
      "1346\n",
      "0.3010929306611844\n",
      "1347\n",
      "0.3010929306611844\n",
      "1348\n",
      "0.3010929306611844\n",
      "1349\n",
      "0.3010929306611844\n",
      "1350\n",
      "0.3010929306611844\n",
      "1351\n",
      "0.3010929306611844\n",
      "1352\n",
      "0.3010929306611844\n",
      "1353\n",
      "0.3010929306611844\n",
      "1354\n",
      "0.3010929306611844\n",
      "1355\n",
      "0.3010929306611844\n",
      "1356\n",
      "0.3010929306611844\n",
      "1357\n",
      "0.3010929306611844\n",
      "1358\n",
      "0.3010929306611844\n",
      "1359\n",
      "0.3010929306611844\n",
      "1360\n",
      "0.3010929306611844\n",
      "1361\n",
      "0.3010929306611844\n",
      "1362\n",
      "0.3010929306611844\n",
      "1363\n",
      "0.3010929306611844\n",
      "1364\n",
      "0.3010929306611844\n",
      "1365\n",
      "0.3010929306611844\n",
      "1366\n",
      "0.3010929306611844\n",
      "1367\n",
      "0.3010929306611844\n",
      "1368\n",
      "0.3010929306611844\n",
      "1369\n",
      "0.3010929306611844\n",
      "1370\n",
      "0.3010929306611844\n",
      "1371\n",
      "0.3010929306611844\n",
      "1372\n",
      "0.3010929306611844\n",
      "1373\n",
      "0.3010929306611844\n",
      "1374\n",
      "0.3010929306611844\n",
      "1375\n",
      "0.3010929306611844\n",
      "1376\n",
      "0.3010929306611844\n",
      "1377\n",
      "0.3010929306611844\n",
      "1378\n",
      "0.3010929306611844\n",
      "1379\n",
      "0.3010929306611844\n",
      "1380\n",
      "0.3010929306611844\n",
      "1381\n",
      "0.3010929306611844\n",
      "1382\n",
      "0.3010929306611844\n",
      "1383\n",
      "0.3010929306611844\n",
      "1384\n",
      "0.3010929306611844\n",
      "1385\n",
      "0.3010929306611844\n",
      "1386\n",
      "0.3010929306611844\n",
      "1387\n",
      "0.3010929306611844\n",
      "1388\n",
      "0.3010929306611844\n",
      "1389\n",
      "0.3010929306611844\n",
      "1390\n",
      "0.3010929306611844\n",
      "1391\n",
      "0.3010929306611844\n",
      "1392\n",
      "0.3010929306611844\n",
      "1393\n",
      "0.3010929306611844\n",
      "1394\n",
      "0.3010929306611844\n",
      "1395\n",
      "0.3010929306611844\n",
      "1396\n",
      "0.3010929306611844\n",
      "1397\n",
      "0.3010929306611844\n",
      "1398\n",
      "0.3010929306611844\n",
      "1399\n",
      "0.3010929306611844\n",
      "1400\n",
      "0.3010929306611844\n",
      "1401\n",
      "0.3010929306611844\n",
      "1402\n",
      "0.3010929306611844\n",
      "1403\n",
      "0.3010929306611844\n",
      "1404\n",
      "0.3010929306611844\n",
      "1405\n",
      "0.3010929306611844\n",
      "1406\n",
      "0.3010929306611844\n",
      "1407\n",
      "0.3010929306611844\n",
      "1408\n",
      "0.3010929306611844\n",
      "1409\n",
      "0.3010929306611844\n",
      "1410\n",
      "0.3010929306611844\n",
      "1411\n",
      "0.3010929306611844\n",
      "1412\n",
      "0.3010929306611844\n",
      "1413\n",
      "0.3010929306611844\n",
      "1414\n",
      "0.3010929306611844\n",
      "1415\n",
      "0.3010929306611844\n",
      "1416\n",
      "0.3010929306611844\n",
      "1417\n",
      "0.3010929306611844\n",
      "1418\n",
      "0.3010929306611844\n",
      "1419\n",
      "0.3010929306611844\n",
      "1420\n",
      "0.3010929306611844\n",
      "1421\n",
      "0.3010929306611844\n",
      "1422\n",
      "0.3010929306611844\n",
      "1423\n",
      "0.3010929306611844\n",
      "1424\n",
      "0.3010929306611844\n",
      "1425\n",
      "0.3010929306611844\n",
      "1426\n",
      "0.3010929306611844\n",
      "1427\n",
      "0.3010929306611844\n",
      "1428\n",
      "0.3010929306611844\n",
      "1429\n",
      "0.3010929306611844\n",
      "1430\n",
      "0.3010929306611844\n",
      "1431\n",
      "0.3010929306611844\n",
      "1432\n",
      "0.3010929306611844\n",
      "1433\n",
      "0.3010929306611844\n",
      "1434\n",
      "0.3010929306611844\n",
      "1435\n",
      "0.3010929306611844\n",
      "1436\n",
      "0.3010929306611844\n",
      "1437\n",
      "0.3010929306611844\n",
      "1438\n",
      "0.3010929306611844\n",
      "1439\n",
      "0.3010929306611844\n",
      "1440\n",
      "0.3010929306611844\n",
      "1441\n",
      "0.3010929306611844\n",
      "1442\n",
      "0.3010929306611844\n",
      "1443\n",
      "0.3010929306611844\n",
      "1444\n",
      "0.3010929306611844\n",
      "1445\n",
      "0.3010929306611844\n",
      "1446\n",
      "0.3010929306611844\n",
      "1447\n",
      "0.3010929306611844\n",
      "1448\n",
      "0.3010929306611844\n",
      "1449\n",
      "0.3010929306611844\n",
      "1450\n",
      "0.3010929306611844\n",
      "1451\n",
      "0.3010929306611844\n",
      "1452\n",
      "0.3010929306611844\n",
      "1453\n",
      "0.3010929306611844\n",
      "1454\n",
      "0.3010929306611844\n",
      "1455\n",
      "0.3010929306611844\n",
      "1456\n",
      "0.3010929306611844\n",
      "1457\n",
      "0.3010929306611844\n",
      "1458\n",
      "0.3010929306611844\n",
      "1459\n",
      "0.3010929306611844\n",
      "1460\n",
      "0.3010929306611844\n",
      "1461\n",
      "0.3010929306611844\n",
      "1462\n",
      "0.3010929306611844\n",
      "1463\n",
      "0.3010929306611844\n",
      "1464\n",
      "0.3010929306611844\n",
      "1465\n",
      "0.3010929306611844\n",
      "1466\n",
      "0.3010929306611844\n",
      "1467\n",
      "0.3010929306611844\n",
      "1468\n",
      "0.3010929306611844\n",
      "1469\n",
      "0.3010929306611844\n",
      "1470\n",
      "0.3010929306611844\n",
      "1471\n",
      "0.3010929306611844\n",
      "1472\n",
      "0.3010929306611844\n",
      "1473\n",
      "0.3010929306611844\n",
      "1474\n",
      "0.3010929306611844\n",
      "1475\n",
      "0.3010929306611844\n",
      "1476\n",
      "0.3010929306611844\n",
      "1477\n",
      "0.3010929306611844\n",
      "1478\n",
      "0.3010929306611844\n",
      "1479\n",
      "0.3010929306611844\n",
      "1480\n",
      "0.3010929306611844\n",
      "1481\n",
      "0.3010929306611844\n",
      "1482\n",
      "0.3010929306611844\n",
      "1483\n",
      "0.3010929306611844\n",
      "1484\n",
      "0.3010929306611844\n",
      "1485\n",
      "0.3010929306611844\n",
      "1486\n",
      "0.3010929306611844\n",
      "1487\n",
      "0.3010929306611844\n",
      "1488\n",
      "0.3010929306611844\n",
      "1489\n",
      "0.3010929306611844\n",
      "1490\n",
      "0.3010929306611844\n",
      "1491\n",
      "0.3010929306611844\n",
      "1492\n",
      "0.3010929306611844\n",
      "1493\n",
      "0.3010929306611844\n",
      "1494\n",
      "0.3010929306611844\n",
      "1495\n",
      "0.3010929306611844\n",
      "1496\n",
      "0.3010929306611844\n",
      "1497\n",
      "0.3010929306611844\n",
      "2936\n",
      "0.3218281472570564\n",
      "2937\n",
      "0.3218281472570564\n",
      "2938\n",
      "0.3218281472570564\n",
      "2939\n",
      "0.3218281472570564\n",
      "2940\n",
      "0.3218281472570564\n",
      "2941\n",
      "0.3218281472570564\n",
      "2942\n",
      "0.3218281472570564\n",
      "2943\n",
      "0.3218281472570564\n",
      "2944\n",
      "0.3218281472570564\n",
      "2945\n",
      "0.3218281472570564\n",
      "2946\n",
      "0.3218281472570564\n",
      "2947\n",
      "0.3218281472570564\n",
      "2948\n",
      "0.3218281472570564\n",
      "2949\n",
      "0.3218281472570564\n",
      "2950\n",
      "0.3218281472570564\n",
      "2951\n",
      "0.3218281472570564\n",
      "2952\n",
      "0.3218281472570564\n",
      "2953\n",
      "0.3218281472570564\n",
      "2954\n",
      "0.3218281472570564\n",
      "2955\n",
      "0.3218281472570564\n",
      "2956\n",
      "0.3218281472570564\n",
      "2957\n",
      "0.3218281472570564\n",
      "2958\n",
      "0.3218281472570564\n",
      "2959\n",
      "0.3218281472570564\n",
      "2960\n",
      "0.3218281472570564\n",
      "2961\n",
      "0.3218281472570564\n",
      "2962\n",
      "0.3218281472570564\n",
      "2963\n",
      "0.3218281472570564\n",
      "2964\n",
      "0.3218281472570564\n",
      "2965\n",
      "0.3218281472570564\n",
      "2966\n",
      "0.3218281472570564\n",
      "2967\n",
      "0.3218281472570564\n",
      "2968\n",
      "0.3218281472570564\n",
      "2969\n",
      "0.3218281472570564\n",
      "2970\n",
      "0.3218281472570564\n",
      "2971\n",
      "0.3218281472570564\n",
      "2972\n",
      "0.3218281472570564\n",
      "2973\n",
      "0.3218281472570564\n",
      "2974\n",
      "0.3218281472570564\n",
      "2975\n",
      "0.3218281472570564\n",
      "2976\n",
      "0.3218281472570564\n",
      "2977\n",
      "0.3218281472570564\n",
      "2978\n",
      "0.3218281472570564\n",
      "2979\n",
      "0.3218281472570564\n",
      "2980\n",
      "0.3218281472570564\n",
      "2981\n",
      "0.3218281472570564\n",
      "2982\n",
      "0.3218281472570564\n",
      "2983\n",
      "0.3218281472570564\n",
      "2984\n",
      "0.3218281472570564\n",
      "2985\n",
      "0.3218281472570564\n",
      "2986\n",
      "0.3218281472570564\n",
      "2987\n",
      "0.3218281472570564\n",
      "2988\n",
      "0.3218281472570564\n",
      "2989\n",
      "0.3218281472570564\n",
      "2990\n",
      "0.3218281472570564\n",
      "2991\n",
      "0.3218281472570564\n",
      "2992\n",
      "0.3218281472570564\n",
      "2993\n",
      "0.3218281472570564\n",
      "2994\n",
      "0.3218281472570564\n",
      "2995\n",
      "0.3218281472570564\n",
      "2996\n",
      "0.3218281472570564\n",
      "2997\n",
      "0.3218281472570564\n",
      "2998\n",
      "0.3218281472570564\n",
      "2999\n",
      "0.3218281472570564\n",
      "3000\n",
      "0.3218281472570564\n",
      "3001\n",
      "0.3218281472570564\n",
      "3002\n",
      "0.3218281472570564\n",
      "3003\n",
      "0.3218281472570564\n",
      "3004\n",
      "0.3218281472570564\n",
      "3005\n",
      "0.3218281472570564\n",
      "3006\n",
      "0.3218281472570564\n",
      "3007\n",
      "0.3218281472570564\n",
      "3008\n",
      "0.3218281472570564\n",
      "3009\n",
      "0.3218281472570564\n",
      "3010\n",
      "0.3218281472570564\n",
      "3011\n",
      "0.3218281472570564\n",
      "3012\n",
      "0.3218281472570564\n",
      "3013\n",
      "0.3218281472570564\n",
      "3014\n",
      "0.3218281472570564\n",
      "3015\n",
      "0.3218281472570564\n",
      "3016\n",
      "0.3218281472570564\n",
      "3017\n",
      "0.3218281472570564\n",
      "3018\n",
      "0.3218281472570564\n",
      "3019\n",
      "0.3218281472570564\n",
      "3020\n",
      "0.3218281472570564\n",
      "3021\n",
      "0.3218281472570564\n",
      "3022\n",
      "0.3218281472570564\n",
      "3023\n",
      "0.3218281472570564\n",
      "3024\n",
      "0.3218281472570564\n",
      "3025\n",
      "0.3218281472570564\n",
      "3026\n",
      "0.3218281472570564\n",
      "3027\n",
      "0.3218281472570564\n",
      "3028\n",
      "0.3218281472570564\n",
      "3029\n",
      "0.3218281472570564\n",
      "3030\n",
      "0.3218281472570564\n",
      "3031\n",
      "0.3218281472570564\n",
      "3032\n",
      "0.3218281472570564\n",
      "3033\n",
      "0.3218281472570564\n",
      "3034\n",
      "0.3218281472570564\n",
      "3035\n",
      "0.3218281472570564\n",
      "3036\n",
      "0.3218281472570564\n",
      "3037\n",
      "0.3218281472570564\n",
      "3038\n",
      "0.3218281472570564\n",
      "3039\n",
      "0.3218281472570564\n",
      "3040\n",
      "0.3218281472570564\n",
      "3041\n",
      "0.3218281472570564\n",
      "3042\n",
      "0.3218281472570564\n",
      "3043\n",
      "0.3218281472570564\n",
      "3044\n",
      "0.3218281472570564\n",
      "3045\n",
      "0.3218281472570564\n",
      "3046\n",
      "0.3218281472570564\n",
      "3047\n",
      "0.3218281472570564\n",
      "3048\n",
      "0.3218281472570564\n",
      "3049\n",
      "0.3218281472570564\n",
      "3050\n",
      "0.3218281472570564\n",
      "3051\n",
      "0.3218281472570564\n",
      "3052\n",
      "0.3218281472570564\n",
      "3053\n",
      "0.3218281472570564\n",
      "3054\n",
      "0.3218281472570564\n",
      "3055\n",
      "0.3218281472570564\n",
      "3056\n",
      "0.3218281472570564\n",
      "3057\n",
      "0.3218281472570564\n",
      "3058\n",
      "0.3218281472570564\n",
      "3059\n",
      "0.3218281472570564\n",
      "3060\n",
      "0.3218281472570564\n",
      "3061\n",
      "0.3218281472570564\n",
      "3062\n",
      "0.3218281472570564\n",
      "3063\n",
      "0.3218281472570564\n",
      "3064\n",
      "0.3218281472570564\n",
      "3065\n",
      "0.3218281472570564\n",
      "3066\n",
      "0.3218281472570564\n",
      "3067\n",
      "0.3218281472570564\n",
      "3068\n",
      "0.3218281472570564\n",
      "3069\n",
      "0.3218281472570564\n",
      "3070\n",
      "0.3218281472570564\n",
      "3071\n",
      "0.3218281472570564\n",
      "3072\n",
      "0.3218281472570564\n",
      "3073\n",
      "0.3218281472570564\n",
      "3074\n",
      "0.3218281472570564\n",
      "3075\n",
      "0.3218281472570564\n",
      "3076\n",
      "0.3218281472570564\n",
      "3077\n",
      "0.3218281472570564\n",
      "3078\n",
      "0.3218281472570564\n",
      "3079\n",
      "0.3218281472570564\n",
      "3080\n",
      "0.3218281472570564\n",
      "3081\n",
      "0.3218281472570564\n",
      "3082\n",
      "0.3218281472570564\n",
      "3083\n",
      "0.3218281472570564\n",
      "3084\n",
      "0.3218281472570564\n",
      "3085\n",
      "0.3218281472570564\n",
      "3086\n",
      "0.3218281472570564\n",
      "3087\n",
      "0.3218281472570564\n",
      "3088\n",
      "0.3218281472570564\n",
      "3089\n",
      "0.3218281472570564\n",
      "3090\n",
      "0.3218281472570564\n",
      "3091\n",
      "0.3218281472570564\n",
      "3092\n",
      "0.3218281472570564\n",
      "3093\n",
      "0.3218281472570564\n",
      "3094\n",
      "0.3218281472570564\n",
      "3095\n",
      "0.3218281472570564\n",
      "3096\n",
      "0.3218281472570564\n",
      "3097\n",
      "0.3218281472570564\n",
      "3098\n",
      "0.3218281472570564\n",
      "3099\n",
      "0.3218281472570564\n",
      "3100\n",
      "0.3218281472570564\n",
      "3101\n",
      "0.3218281472570564\n",
      "3102\n",
      "0.3218281472570564\n",
      "3103\n",
      "0.3218281472570564\n",
      "3104\n",
      "0.3218281472570564\n",
      "3105\n",
      "0.3218281472570564\n",
      "3106\n",
      "0.3218281472570564\n",
      "3107\n",
      "0.3218281472570564\n",
      "3108\n",
      "0.3218281472570564\n",
      "3109\n",
      "0.3218281472570564\n",
      "3110\n",
      "0.3218281472570564\n",
      "3111\n",
      "0.3218281472570564\n",
      "3112\n",
      "0.3218281472570564\n",
      "3113\n",
      "0.3218281472570564\n",
      "3114\n",
      "0.3218281472570564\n",
      "3115\n",
      "0.3218281472570564\n",
      "3116\n",
      "0.3218281472570564\n",
      "3117\n",
      "0.3218281472570564\n",
      "3118\n",
      "0.3218281472570564\n",
      "3119\n",
      "0.3218281472570564\n",
      "3120\n",
      "0.3218281472570564\n",
      "3121\n",
      "0.3218281472570564\n",
      "3122\n",
      "0.3218281472570564\n",
      "3123\n",
      "0.3218281472570564\n",
      "3124\n",
      "0.3218281472570564\n",
      "3125\n",
      "0.3218281472570564\n",
      "3126\n",
      "0.3218281472570564\n",
      "3127\n",
      "0.3218281472570564\n",
      "3128\n",
      "0.3218281472570564\n",
      "3129\n",
      "0.3218281472570564\n",
      "3130\n",
      "0.3218281472570564\n",
      "3131\n",
      "0.3218281472570564\n",
      "3132\n",
      "0.3218281472570564\n",
      "3133\n",
      "0.3218281472570564\n",
      "3134\n",
      "0.3218281472570564\n",
      "3135\n",
      "0.3218281472570564\n",
      "3136\n",
      "0.3218281472570564\n",
      "3137\n",
      "0.3218281472570564\n",
      "3138\n",
      "0.3218281472570564\n",
      "3139\n",
      "0.3218281472570564\n",
      "3140\n",
      "0.3218281472570564\n",
      "3141\n",
      "0.3218281472570564\n",
      "3142\n",
      "0.3218281472570564\n",
      "3143\n",
      "0.3218281472570564\n",
      "3144\n",
      "0.3218281472570564\n",
      "3145\n",
      "0.3218281472570564\n",
      "3146\n",
      "0.3218281472570564\n",
      "3147\n",
      "0.3218281472570564\n",
      "3148\n",
      "0.3218281472570564\n",
      "3149\n",
      "0.3218281472570564\n",
      "3150\n",
      "0.3218281472570564\n",
      "3151\n",
      "0.3218281472570564\n",
      "3152\n",
      "0.3218281472570564\n",
      "3153\n",
      "0.3218281472570564\n",
      "3154\n",
      "0.3218281472570564\n",
      "3155\n",
      "0.3218281472570564\n",
      "3156\n",
      "0.3218281472570564\n",
      "3157\n",
      "0.3218281472570564\n",
      "3158\n",
      "0.3218281472570564\n",
      "3159\n",
      "0.3218281472570564\n",
      "3160\n",
      "0.3218281472570564\n",
      "3161\n",
      "0.3218281472570564\n",
      "3162\n",
      "0.3218281472570564\n",
      "3163\n",
      "0.3218281472570564\n",
      "3164\n",
      "0.3218281472570564\n",
      "3165\n",
      "0.3218281472570564\n",
      "3166\n",
      "0.3218281472570564\n",
      "3167\n",
      "0.3218281472570564\n",
      "3168\n",
      "0.3218281472570564\n",
      "3169\n",
      "0.3218281472570564\n",
      "3170\n",
      "0.3218281472570564\n",
      "3171\n",
      "0.3218281472570564\n",
      "3172\n",
      "0.3218281472570564\n",
      "3173\n",
      "0.3218281472570564\n",
      "3174\n",
      "0.3218281472570564\n",
      "3175\n",
      "0.3218281472570564\n",
      "3176\n",
      "0.3218281472570564\n",
      "3177\n",
      "0.3218281472570564\n",
      "3178\n",
      "0.3218281472570564\n",
      "3179\n",
      "0.3218281472570564\n",
      "3180\n",
      "0.3218281472570564\n",
      "3181\n",
      "0.3218281472570564\n",
      "3182\n",
      "0.3218281472570564\n",
      "3183\n",
      "0.3218281472570564\n",
      "3184\n",
      "0.3218281472570564\n",
      "3185\n",
      "0.3218281472570564\n",
      "3186\n",
      "0.3218281472570564\n",
      "3187\n",
      "0.3218281472570564\n",
      "3188\n",
      "0.3218281472570564\n",
      "3189\n",
      "0.3218281472570564\n",
      "3190\n",
      "0.3218281472570564\n",
      "3191\n",
      "0.3218281472570564\n",
      "3192\n",
      "0.3218281472570564\n",
      "3193\n",
      "0.3218281472570564\n",
      "3194\n",
      "0.3218281472570564\n",
      "3195\n",
      "0.3218281472570564\n",
      "3196\n",
      "0.3218281472570564\n",
      "3197\n",
      "0.3218281472570564\n",
      "3198\n",
      "0.3218281472570564\n",
      "3199\n",
      "0.3218281472570564\n",
      "3200\n",
      "0.3218281472570564\n",
      "3201\n",
      "0.3218281472570564\n",
      "3202\n",
      "0.3218281472570564\n",
      "3203\n",
      "0.3218281472570564\n",
      "3204\n",
      "0.3218281472570564\n",
      "3205\n",
      "0.3218281472570564\n",
      "3206\n",
      "0.3218281472570564\n",
      "3207\n",
      "0.3218281472570564\n",
      "3208\n",
      "0.3218281472570564\n",
      "3209\n",
      "0.3218281472570564\n",
      "3210\n",
      "0.3218281472570564\n",
      "3211\n",
      "0.3218281472570564\n",
      "3212\n",
      "0.3218281472570564\n",
      "3213\n",
      "0.3218281472570564\n",
      "3214\n",
      "0.3218281472570564\n",
      "3215\n",
      "0.3218281472570564\n",
      "3216\n",
      "0.3218281472570564\n",
      "3217\n",
      "0.3218281472570564\n",
      "3218\n",
      "0.3218281472570564\n",
      "3219\n",
      "0.3218281472570564\n",
      "3220\n",
      "0.3218281472570564\n",
      "3221\n",
      "0.3218281472570564\n",
      "3222\n",
      "0.3218281472570564\n",
      "5542\n",
      "0.26416439506645106\n",
      "5543\n",
      "0.26416439506645106\n",
      "5544\n",
      "0.26416439506645106\n",
      "5545\n",
      "0.26416439506645106\n",
      "5546\n",
      "0.26416439506645106\n",
      "5547\n",
      "0.26416439506645106\n",
      "5548\n",
      "0.26416439506645106\n",
      "5549\n",
      "0.26416439506645106\n",
      "5550\n",
      "0.26416439506645106\n",
      "5551\n",
      "0.26416439506645106\n",
      "5552\n",
      "0.26416439506645106\n",
      "5553\n",
      "0.26416439506645106\n",
      "5554\n",
      "0.26416439506645106\n",
      "5555\n",
      "0.26416439506645106\n",
      "5556\n",
      "0.26416439506645106\n",
      "5557\n",
      "0.26416439506645106\n",
      "5558\n",
      "0.26416439506645106\n",
      "5559\n",
      "0.26416439506645106\n",
      "5560\n",
      "0.26416439506645106\n",
      "5561\n",
      "0.26416439506645106\n",
      "5562\n",
      "0.26416439506645106\n",
      "5563\n",
      "0.26416439506645106\n",
      "5564\n",
      "0.26416439506645106\n",
      "5565\n",
      "0.26416439506645106\n",
      "5566\n",
      "0.26416439506645106\n",
      "5567\n",
      "0.26416439506645106\n",
      "5568\n",
      "0.26416439506645106\n",
      "5569\n",
      "0.26416439506645106\n",
      "5570\n",
      "0.26416439506645106\n",
      "5571\n",
      "0.26416439506645106\n",
      "5572\n",
      "0.26416439506645106\n",
      "5573\n",
      "0.26416439506645106\n",
      "5574\n",
      "0.26416439506645106\n",
      "5575\n",
      "0.26416439506645106\n",
      "5576\n",
      "0.26416439506645106\n",
      "5577\n",
      "0.26416439506645106\n",
      "5578\n",
      "0.26416439506645106\n",
      "5579\n",
      "0.26416439506645106\n",
      "5580\n",
      "0.26416439506645106\n",
      "5581\n",
      "0.26416439506645106\n",
      "5582\n",
      "0.26416439506645106\n",
      "5583\n",
      "0.26416439506645106\n",
      "5584\n",
      "0.26416439506645106\n",
      "5585\n",
      "0.26416439506645106\n",
      "5586\n",
      "0.26416439506645106\n",
      "5587\n",
      "0.26416439506645106\n",
      "5588\n",
      "0.26416439506645106\n",
      "5589\n",
      "0.26416439506645106\n",
      "5590\n",
      "0.26416439506645106\n",
      "5591\n",
      "0.26416439506645106\n",
      "5592\n",
      "0.26416439506645106\n",
      "5593\n",
      "0.26416439506645106\n",
      "5594\n",
      "0.26416439506645106\n",
      "5595\n",
      "0.26416439506645106\n",
      "5596\n",
      "0.26416439506645106\n",
      "5597\n",
      "0.26416439506645106\n",
      "5598\n",
      "0.26416439506645106\n",
      "5599\n",
      "0.26416439506645106\n",
      "5600\n",
      "0.26416439506645106\n",
      "5601\n",
      "0.26416439506645106\n",
      "5602\n",
      "0.26416439506645106\n",
      "5603\n",
      "0.26416439506645106\n",
      "5604\n",
      "0.26416439506645106\n",
      "5605\n",
      "0.26416439506645106\n",
      "5606\n",
      "0.26416439506645106\n",
      "5607\n",
      "0.26416439506645106\n",
      "5608\n",
      "0.26416439506645106\n",
      "5609\n",
      "0.26416439506645106\n",
      "5610\n",
      "0.26416439506645106\n",
      "5611\n",
      "0.26416439506645106\n",
      "5612\n",
      "0.26416439506645106\n",
      "5613\n",
      "0.26416439506645106\n",
      "5614\n",
      "0.26416439506645106\n",
      "5615\n",
      "0.26416439506645106\n",
      "5616\n",
      "0.26416439506645106\n",
      "5617\n",
      "0.26416439506645106\n",
      "5618\n",
      "0.26416439506645106\n",
      "5619\n",
      "0.26416439506645106\n",
      "5620\n",
      "0.26416439506645106\n",
      "5621\n",
      "0.26416439506645106\n",
      "5622\n",
      "0.26416439506645106\n",
      "5623\n",
      "0.26416439506645106\n",
      "5624\n",
      "0.26416439506645106\n",
      "5625\n",
      "0.26416439506645106\n",
      "5626\n",
      "0.26416439506645106\n",
      "5627\n",
      "0.26416439506645106\n",
      "5628\n",
      "0.26416439506645106\n",
      "5629\n",
      "0.26416439506645106\n",
      "5630\n",
      "0.26416439506645106\n",
      "5631\n",
      "0.26416439506645106\n",
      "5632\n",
      "0.26416439506645106\n",
      "5633\n",
      "0.26416439506645106\n",
      "5634\n",
      "0.26416439506645106\n",
      "5635\n",
      "0.26416439506645106\n",
      "5636\n",
      "0.26416439506645106\n",
      "5637\n",
      "0.26416439506645106\n",
      "5638\n",
      "0.26416439506645106\n",
      "5639\n",
      "0.26416439506645106\n",
      "5640\n",
      "0.26416439506645106\n",
      "5641\n",
      "0.26416439506645106\n",
      "5642\n",
      "0.26416439506645106\n",
      "5643\n",
      "0.26416439506645106\n",
      "5644\n",
      "0.26416439506645106\n",
      "5645\n",
      "0.26416439506645106\n",
      "5646\n",
      "0.26416439506645106\n",
      "5647\n",
      "0.26416439506645106\n",
      "5648\n",
      "0.26416439506645106\n",
      "5649\n",
      "0.26416439506645106\n",
      "5650\n",
      "0.26416439506645106\n",
      "5651\n",
      "0.26416439506645106\n",
      "5652\n",
      "0.26416439506645106\n",
      "5653\n",
      "0.26416439506645106\n",
      "5654\n",
      "0.26416439506645106\n",
      "5655\n",
      "0.26416439506645106\n",
      "5656\n",
      "0.26416439506645106\n",
      "5657\n",
      "0.26416439506645106\n",
      "5658\n",
      "0.26416439506645106\n",
      "5659\n",
      "0.26416439506645106\n",
      "5660\n",
      "0.26416439506645106\n",
      "5661\n",
      "0.26416439506645106\n",
      "5662\n",
      "0.26416439506645106\n",
      "5663\n",
      "0.26416439506645106\n",
      "5664\n",
      "0.26416439506645106\n",
      "5665\n",
      "0.26416439506645106\n",
      "5666\n",
      "0.26416439506645106\n",
      "5667\n",
      "0.26416439506645106\n",
      "5668\n",
      "0.26416439506645106\n",
      "5669\n",
      "0.26416439506645106\n",
      "5670\n",
      "0.26416439506645106\n",
      "5671\n",
      "0.26416439506645106\n",
      "5672\n",
      "0.26416439506645106\n",
      "5673\n",
      "0.26416439506645106\n",
      "5674\n",
      "0.26416439506645106\n",
      "5675\n",
      "0.26416439506645106\n",
      "5676\n",
      "0.26416439506645106\n",
      "5677\n",
      "0.26416439506645106\n",
      "5678\n",
      "0.26416439506645106\n",
      "5679\n",
      "0.26416439506645106\n",
      "5680\n",
      "0.26416439506645106\n",
      "5681\n",
      "0.26416439506645106\n",
      "5682\n",
      "0.26416439506645106\n",
      "5683\n",
      "0.26416439506645106\n",
      "5684\n",
      "0.26416439506645106\n",
      "5685\n",
      "0.26416439506645106\n",
      "5686\n",
      "0.26416439506645106\n",
      "5687\n",
      "0.26416439506645106\n",
      "5688\n",
      "0.26416439506645106\n",
      "5689\n",
      "0.26416439506645106\n",
      "5690\n",
      "0.26416439506645106\n",
      "5691\n",
      "0.26416439506645106\n",
      "5692\n",
      "0.26416439506645106\n",
      "5693\n",
      "0.26416439506645106\n",
      "5694\n",
      "0.26416439506645106\n",
      "5695\n",
      "0.26416439506645106\n",
      "5696\n",
      "0.26416439506645106\n",
      "5697\n",
      "0.26416439506645106\n",
      "5698\n",
      "0.26416439506645106\n",
      "5699\n",
      "0.26416439506645106\n",
      "5700\n",
      "0.26416439506645106\n",
      "5701\n",
      "0.26416439506645106\n",
      "5702\n",
      "0.26416439506645106\n",
      "5703\n",
      "0.26416439506645106\n",
      "5704\n",
      "0.26416439506645106\n",
      "5705\n",
      "0.26416439506645106\n",
      "5706\n",
      "0.26416439506645106\n",
      "5707\n",
      "0.26416439506645106\n",
      "5708\n",
      "0.26416439506645106\n",
      "5709\n",
      "0.26416439506645106\n",
      "5710\n",
      "0.26416439506645106\n",
      "5711\n",
      "0.26416439506645106\n",
      "5712\n",
      "0.26416439506645106\n",
      "5713\n",
      "0.26416439506645106\n",
      "5714\n",
      "0.26416439506645106\n",
      "5715\n",
      "0.26416439506645106\n",
      "5716\n",
      "0.26416439506645106\n",
      "5717\n",
      "0.26416439506645106\n",
      "5718\n",
      "0.26416439506645106\n",
      "5719\n",
      "0.26416439506645106\n",
      "5720\n",
      "0.26416439506645106\n",
      "5721\n",
      "0.26416439506645106\n",
      "5722\n",
      "0.26416439506645106\n",
      "5723\n",
      "0.26416439506645106\n",
      "5724\n",
      "0.26416439506645106\n",
      "5725\n",
      "0.26416439506645106\n",
      "5726\n",
      "0.26416439506645106\n",
      "5727\n",
      "0.26416439506645106\n",
      "5728\n",
      "0.26416439506645106\n",
      "5729\n",
      "0.26416439506645106\n",
      "5730\n",
      "0.26416439506645106\n",
      "5731\n",
      "0.26416439506645106\n",
      "5732\n",
      "0.26416439506645106\n",
      "5733\n",
      "0.26416439506645106\n",
      "5734\n",
      "0.26416439506645106\n",
      "5735\n",
      "0.26416439506645106\n",
      "5736\n",
      "0.26416439506645106\n",
      "5737\n",
      "0.26416439506645106\n",
      "5738\n",
      "0.26416439506645106\n",
      "5739\n",
      "0.26416439506645106\n",
      "5740\n",
      "0.26416439506645106\n",
      "5741\n",
      "0.26416439506645106\n",
      "5742\n",
      "0.26416439506645106\n",
      "5743\n",
      "0.26416439506645106\n",
      "5744\n",
      "0.26416439506645106\n",
      "5745\n",
      "0.26416439506645106\n",
      "5746\n",
      "0.26416439506645106\n",
      "5747\n",
      "0.26416439506645106\n",
      "5748\n",
      "0.26416439506645106\n",
      "5749\n",
      "0.26416439506645106\n",
      "5750\n",
      "0.26416439506645106\n",
      "5751\n",
      "0.26416439506645106\n",
      "5752\n",
      "0.26416439506645106\n",
      "5753\n",
      "0.26416439506645106\n",
      "5754\n",
      "0.26416439506645106\n",
      "5755\n",
      "0.26416439506645106\n",
      "5756\n",
      "0.26416439506645106\n",
      "5757\n",
      "0.26416439506645106\n",
      "5758\n",
      "0.26416439506645106\n",
      "5759\n",
      "0.26416439506645106\n",
      "5760\n",
      "0.26416439506645106\n",
      "5761\n",
      "0.26416439506645106\n",
      "5762\n",
      "0.26416439506645106\n",
      "5763\n",
      "0.26416439506645106\n",
      "5764\n",
      "0.26416439506645106\n",
      "5765\n",
      "0.26416439506645106\n",
      "5766\n",
      "0.26416439506645106\n",
      "5767\n",
      "0.26416439506645106\n",
      "5768\n",
      "0.26416439506645106\n",
      "5769\n",
      "0.26416439506645106\n",
      "5770\n",
      "0.26416439506645106\n",
      "5771\n",
      "0.26416439506645106\n",
      "5772\n",
      "0.26416439506645106\n",
      "5773\n",
      "0.26416439506645106\n",
      "5774\n",
      "0.26416439506645106\n",
      "5775\n",
      "0.26416439506645106\n",
      "5776\n",
      "0.26416439506645106\n",
      "5777\n",
      "0.26416439506645106\n",
      "5778\n",
      "0.26416439506645106\n",
      "5779\n",
      "0.26416439506645106\n",
      "5780\n",
      "0.26416439506645106\n",
      "5781\n",
      "0.26416439506645106\n",
      "5782\n",
      "0.26416439506645106\n",
      "5783\n",
      "0.26416439506645106\n",
      "5784\n",
      "0.26416439506645106\n",
      "5785\n",
      "0.26416439506645106\n",
      "5786\n",
      "0.26416439506645106\n",
      "5787\n",
      "0.26416439506645106\n",
      "5788\n",
      "0.26416439506645106\n",
      "5789\n",
      "0.26416439506645106\n",
      "5790\n",
      "0.26416439506645106\n",
      "5791\n",
      "0.26416439506645106\n",
      "5792\n",
      "0.26416439506645106\n",
      "5793\n",
      "0.26416439506645106\n",
      "5794\n",
      "0.26416439506645106\n",
      "5795\n",
      "0.26416439506645106\n",
      "5796\n",
      "0.26416439506645106\n",
      "5797\n",
      "0.26416439506645106\n",
      "5798\n",
      "0.26416439506645106\n",
      "5799\n",
      "0.26416439506645106\n",
      "5800\n",
      "0.26416439506645106\n",
      "5801\n",
      "0.26416439506645106\n",
      "5802\n",
      "0.26416439506645106\n",
      "5803\n",
      "0.26416439506645106\n",
      "5804\n",
      "0.26416439506645106\n",
      "5805\n",
      "0.26416439506645106\n",
      "5806\n",
      "0.26416439506645106\n",
      "5807\n",
      "0.26416439506645106\n",
      "5808\n",
      "0.26416439506645106\n",
      "5809\n",
      "0.26416439506645106\n",
      "5810\n",
      "0.26416439506645106\n",
      "5811\n",
      "0.26416439506645106\n",
      "5812\n",
      "0.26416439506645106\n",
      "5813\n",
      "0.26416439506645106\n",
      "5814\n",
      "0.26416439506645106\n",
      "5815\n",
      "0.26416439506645106\n",
      "5816\n",
      "0.26416439506645106\n",
      "5817\n",
      "0.26416439506645106\n",
      "5818\n",
      "0.26416439506645106\n",
      "5819\n",
      "0.26416439506645106\n",
      "5820\n",
      "0.26416439506645106\n",
      "5821\n",
      "0.26416439506645106\n",
      "5822\n",
      "0.26416439506645106\n",
      "5823\n",
      "0.26416439506645106\n",
      "5824\n",
      "0.26416439506645106\n",
      "5825\n",
      "0.26416439506645106\n",
      "5826\n",
      "0.26416439506645106\n",
      "5827\n",
      "0.26416439506645106\n",
      "5828\n",
      "0.26416439506645106\n",
      "5829\n",
      "0.26416439506645106\n",
      "5830\n",
      "0.2761605775441243\n",
      "5831\n",
      "0.2761605775441243\n",
      "5832\n",
      "0.2761605775441243\n",
      "5833\n",
      "0.2761605775441243\n",
      "5834\n",
      "0.2761605775441243\n",
      "5835\n",
      "0.2761605775441243\n",
      "5836\n",
      "0.2761605775441243\n",
      "5837\n",
      "0.2761605775441243\n",
      "5838\n",
      "0.2761605775441243\n",
      "5839\n",
      "0.2761605775441243\n",
      "5840\n",
      "0.2761605775441243\n",
      "5841\n",
      "0.2761605775441243\n",
      "5842\n",
      "0.2761605775441243\n",
      "5843\n",
      "0.2761605775441243\n",
      "5844\n",
      "0.2761605775441243\n",
      "5845\n",
      "0.2761605775441243\n",
      "5846\n",
      "0.2761605775441243\n",
      "5847\n",
      "0.2761605775441243\n",
      "5848\n",
      "0.2761605775441243\n",
      "5849\n",
      "0.2761605775441243\n",
      "5850\n",
      "0.2761605775441243\n",
      "5851\n",
      "0.2761605775441243\n",
      "5852\n",
      "0.2761605775441243\n",
      "5853\n",
      "0.2761605775441243\n",
      "5854\n",
      "0.2761605775441243\n",
      "5855\n",
      "0.2761605775441243\n",
      "5856\n",
      "0.2761605775441243\n",
      "5857\n",
      "0.2761605775441243\n",
      "5858\n",
      "0.2761605775441243\n",
      "5859\n",
      "0.2761605775441243\n",
      "5860\n",
      "0.2761605775441243\n",
      "5861\n",
      "0.2761605775441243\n",
      "5862\n",
      "0.2761605775441243\n",
      "5863\n",
      "0.2761605775441243\n",
      "5864\n",
      "0.2761605775441243\n",
      "5865\n",
      "0.2761605775441243\n",
      "5866\n",
      "0.2761605775441243\n",
      "5867\n",
      "0.2761605775441243\n",
      "5868\n",
      "0.2761605775441243\n",
      "5869\n",
      "0.2761605775441243\n",
      "5870\n",
      "0.2761605775441243\n",
      "5871\n",
      "0.2761605775441243\n",
      "5872\n",
      "0.2761605775441243\n",
      "5873\n",
      "0.2761605775441243\n",
      "5874\n",
      "0.2761605775441243\n",
      "5875\n",
      "0.2761605775441243\n",
      "5876\n",
      "0.2761605775441243\n",
      "5877\n",
      "0.2761605775441243\n",
      "5878\n",
      "0.2761605775441243\n",
      "5879\n",
      "0.2761605775441243\n",
      "5880\n",
      "0.2761605775441243\n",
      "5881\n",
      "0.2761605775441243\n",
      "5882\n",
      "0.2761605775441243\n",
      "5883\n",
      "0.2761605775441243\n",
      "5884\n",
      "0.2761605775441243\n",
      "5885\n",
      "0.2761605775441243\n",
      "5886\n",
      "0.2761605775441243\n",
      "5887\n",
      "0.2761605775441243\n",
      "5888\n",
      "0.2761605775441243\n",
      "5889\n",
      "0.2761605775441243\n",
      "5890\n",
      "0.2761605775441243\n",
      "5891\n",
      "0.2761605775441243\n",
      "5892\n",
      "0.2761605775441243\n",
      "5893\n",
      "0.2761605775441243\n",
      "5894\n",
      "0.2761605775441243\n",
      "5895\n",
      "0.2761605775441243\n",
      "5896\n",
      "0.2761605775441243\n",
      "5897\n",
      "0.2761605775441243\n",
      "5898\n",
      "0.2761605775441243\n",
      "5899\n",
      "0.2761605775441243\n",
      "5900\n",
      "0.2761605775441243\n",
      "5901\n",
      "0.2761605775441243\n",
      "5902\n",
      "0.2761605775441243\n",
      "5903\n",
      "0.2761605775441243\n",
      "5904\n",
      "0.2761605775441243\n",
      "5905\n",
      "0.2761605775441243\n",
      "5906\n",
      "0.2761605775441243\n",
      "5907\n",
      "0.2761605775441243\n",
      "5908\n",
      "0.2761605775441243\n",
      "5909\n",
      "0.2761605775441243\n",
      "5910\n",
      "0.2761605775441243\n",
      "5911\n",
      "0.2761605775441243\n",
      "5912\n",
      "0.2761605775441243\n",
      "5913\n",
      "0.2761605775441243\n",
      "5914\n",
      "0.2761605775441243\n",
      "5915\n",
      "0.2761605775441243\n",
      "5916\n",
      "0.2761605775441243\n",
      "5917\n",
      "0.2761605775441243\n",
      "5918\n",
      "0.2761605775441243\n",
      "5919\n",
      "0.2761605775441243\n",
      "5920\n",
      "0.2761605775441243\n",
      "5921\n",
      "0.2761605775441243\n",
      "5922\n",
      "0.2761605775441243\n",
      "5923\n",
      "0.2761605775441243\n",
      "5924\n",
      "0.2761605775441243\n",
      "5925\n",
      "0.2761605775441243\n",
      "5926\n",
      "0.2761605775441243\n",
      "5927\n",
      "0.2761605775441243\n",
      "5928\n",
      "0.2761605775441243\n",
      "5929\n",
      "0.2761605775441243\n",
      "5930\n",
      "0.2761605775441243\n",
      "5931\n",
      "0.2761605775441243\n",
      "5932\n",
      "0.2761605775441243\n",
      "5933\n",
      "0.2761605775441243\n",
      "5934\n",
      "0.2761605775441243\n",
      "5935\n",
      "0.2761605775441243\n",
      "5936\n",
      "0.2761605775441243\n",
      "5937\n",
      "0.2761605775441243\n",
      "5938\n",
      "0.2761605775441243\n",
      "5939\n",
      "0.2761605775441243\n",
      "5940\n",
      "0.2761605775441243\n",
      "5941\n",
      "0.2761605775441243\n",
      "5942\n",
      "0.2761605775441243\n",
      "5943\n",
      "0.2761605775441243\n",
      "5944\n",
      "0.2761605775441243\n",
      "5945\n",
      "0.2761605775441243\n",
      "5946\n",
      "0.2761605775441243\n",
      "5947\n",
      "0.2761605775441243\n",
      "5948\n",
      "0.2761605775441243\n",
      "5949\n",
      "0.2761605775441243\n",
      "5950\n",
      "0.2761605775441243\n",
      "5951\n",
      "0.2761605775441243\n",
      "5952\n",
      "0.2761605775441243\n",
      "5953\n",
      "0.2761605775441243\n",
      "5954\n",
      "0.2761605775441243\n",
      "5955\n",
      "0.2761605775441243\n",
      "5956\n",
      "0.2761605775441243\n",
      "5957\n",
      "0.2761605775441243\n",
      "5958\n",
      "0.2761605775441243\n",
      "5959\n",
      "0.2761605775441243\n",
      "5960\n",
      "0.2761605775441243\n",
      "5961\n",
      "0.2761605775441243\n",
      "5962\n",
      "0.2761605775441243\n",
      "5963\n",
      "0.2761605775441243\n",
      "5964\n",
      "0.2761605775441243\n",
      "5965\n",
      "0.2761605775441243\n",
      "5966\n",
      "0.2761605775441243\n",
      "5967\n",
      "0.2761605775441243\n",
      "5968\n",
      "0.2761605775441243\n",
      "5969\n",
      "0.2761605775441243\n",
      "5970\n",
      "0.2761605775441243\n",
      "5971\n",
      "0.2761605775441243\n",
      "5972\n",
      "0.2761605775441243\n",
      "5973\n",
      "0.2761605775441243\n",
      "5974\n",
      "0.2761605775441243\n",
      "5975\n",
      "0.2761605775441243\n",
      "5976\n",
      "0.2761605775441243\n",
      "5977\n",
      "0.2761605775441243\n",
      "5978\n",
      "0.2761605775441243\n",
      "5979\n",
      "0.2761605775441243\n",
      "5980\n",
      "0.2761605775441243\n",
      "5981\n",
      "0.2761605775441243\n",
      "5982\n",
      "0.2761605775441243\n",
      "5983\n",
      "0.2761605775441243\n",
      "5984\n",
      "0.2761605775441243\n",
      "5985\n",
      "0.2761605775441243\n",
      "5986\n",
      "0.2761605775441243\n",
      "5987\n",
      "0.2761605775441243\n",
      "5988\n",
      "0.2761605775441243\n",
      "5989\n",
      "0.2761605775441243\n",
      "5990\n",
      "0.2761605775441243\n",
      "5991\n",
      "0.2761605775441243\n",
      "5992\n",
      "0.2761605775441243\n",
      "5993\n",
      "0.2761605775441243\n",
      "5994\n",
      "0.2761605775441243\n",
      "5995\n",
      "0.2761605775441243\n",
      "5996\n",
      "0.2761605775441243\n",
      "5997\n",
      "0.2761605775441243\n",
      "5998\n",
      "0.2761605775441243\n",
      "5999\n",
      "0.2761605775441243\n",
      "6000\n",
      "0.2761605775441243\n",
      "6001\n",
      "0.2761605775441243\n",
      "6002\n",
      "0.2761605775441243\n",
      "6003\n",
      "0.2761605775441243\n",
      "6004\n",
      "0.2761605775441243\n",
      "6005\n",
      "0.2761605775441243\n",
      "6006\n",
      "0.2761605775441243\n",
      "6007\n",
      "0.2761605775441243\n",
      "6008\n",
      "0.2761605775441243\n",
      "6009\n",
      "0.2761605775441243\n",
      "6010\n",
      "0.2761605775441243\n",
      "6011\n",
      "0.2761605775441243\n",
      "6012\n",
      "0.2761605775441243\n",
      "6013\n",
      "0.2761605775441243\n",
      "6014\n",
      "0.2761605775441243\n",
      "6015\n",
      "0.2761605775441243\n",
      "6016\n",
      "0.2761605775441243\n",
      "6017\n",
      "0.2761605775441243\n",
      "6018\n",
      "0.2761605775441243\n",
      "6019\n",
      "0.2761605775441243\n",
      "6020\n",
      "0.2761605775441243\n",
      "6021\n",
      "0.2761605775441243\n",
      "6022\n",
      "0.2761605775441243\n",
      "6023\n",
      "0.2761605775441243\n",
      "6024\n",
      "0.2761605775441243\n",
      "6025\n",
      "0.2761605775441243\n",
      "6026\n",
      "0.2761605775441243\n",
      "6027\n",
      "0.2761605775441243\n",
      "6028\n",
      "0.2761605775441243\n",
      "6029\n",
      "0.2761605775441243\n",
      "6030\n",
      "0.2761605775441243\n",
      "6031\n",
      "0.2761605775441243\n",
      "6032\n",
      "0.2761605775441243\n",
      "6033\n",
      "0.2761605775441243\n",
      "6034\n",
      "0.2761605775441243\n",
      "6035\n",
      "0.2761605775441243\n",
      "6036\n",
      "0.2761605775441243\n",
      "6037\n",
      "0.2761605775441243\n",
      "6038\n",
      "0.2761605775441243\n",
      "6039\n",
      "0.2761605775441243\n",
      "6040\n",
      "0.2761605775441243\n",
      "6041\n",
      "0.2761605775441243\n",
      "6042\n",
      "0.2761605775441243\n",
      "6043\n",
      "0.2761605775441243\n",
      "6044\n",
      "0.2761605775441243\n",
      "6045\n",
      "0.2761605775441243\n",
      "6046\n",
      "0.2761605775441243\n",
      "6047\n",
      "0.2761605775441243\n",
      "6048\n",
      "0.2761605775441243\n",
      "6049\n",
      "0.2761605775441243\n",
      "6050\n",
      "0.2761605775441243\n",
      "6051\n",
      "0.2761605775441243\n",
      "6052\n",
      "0.2761605775441243\n",
      "6053\n",
      "0.2761605775441243\n",
      "6054\n",
      "0.2761605775441243\n",
      "6055\n",
      "0.2761605775441243\n",
      "6056\n",
      "0.2761605775441243\n",
      "6057\n",
      "0.2761605775441243\n",
      "6058\n",
      "0.2761605775441243\n",
      "6059\n",
      "0.2761605775441243\n",
      "6060\n",
      "0.2761605775441243\n",
      "6061\n",
      "0.2761605775441243\n",
      "6062\n",
      "0.2761605775441243\n",
      "6063\n",
      "0.2761605775441243\n",
      "6064\n",
      "0.2761605775441243\n",
      "6065\n",
      "0.2761605775441243\n",
      "6066\n",
      "0.2761605775441243\n",
      "6067\n",
      "0.2761605775441243\n",
      "6068\n",
      "0.2761605775441243\n",
      "6069\n",
      "0.2761605775441243\n",
      "6070\n",
      "0.2761605775441243\n",
      "6071\n",
      "0.2761605775441243\n",
      "6072\n",
      "0.2761605775441243\n",
      "6073\n",
      "0.2761605775441243\n",
      "6074\n",
      "0.2761605775441243\n",
      "6075\n",
      "0.2761605775441243\n",
      "6076\n",
      "0.2761605775441243\n",
      "6077\n",
      "0.2761605775441243\n",
      "6078\n",
      "0.2761605775441243\n",
      "6079\n",
      "0.2761605775441243\n",
      "6080\n",
      "0.2761605775441243\n",
      "6081\n",
      "0.2761605775441243\n",
      "6082\n",
      "0.2761605775441243\n",
      "6083\n",
      "0.2761605775441243\n",
      "6084\n",
      "0.2761605775441243\n",
      "6085\n",
      "0.2761605775441243\n",
      "6086\n",
      "0.2761605775441243\n",
      "6087\n",
      "0.2761605775441243\n",
      "6088\n",
      "0.2761605775441243\n",
      "6089\n",
      "0.2761605775441243\n",
      "6090\n",
      "0.2761605775441243\n",
      "6091\n",
      "0.2761605775441243\n",
      "6092\n",
      "0.2761605775441243\n",
      "6093\n",
      "0.2761605775441243\n",
      "6094\n",
      "0.2761605775441243\n",
      "6095\n",
      "0.2761605775441243\n",
      "6096\n",
      "0.2761605775441243\n",
      "6097\n",
      "0.2761605775441243\n",
      "6098\n",
      "0.2761605775441243\n",
      "6099\n",
      "0.2761605775441243\n",
      "6100\n",
      "0.2761605775441243\n",
      "6101\n",
      "0.2761605775441243\n",
      "6102\n",
      "0.2761605775441243\n",
      "6103\n",
      "0.2761605775441243\n",
      "6104\n",
      "0.2761605775441243\n",
      "6105\n",
      "0.2761605775441243\n",
      "6106\n",
      "0.2761605775441243\n",
      "6107\n",
      "0.2761605775441243\n",
      "6108\n",
      "0.2761605775441243\n",
      "6109\n",
      "0.2761605775441243\n",
      "6110\n",
      "0.2761605775441243\n",
      "6111\n",
      "0.2761605775441243\n",
      "6112\n",
      "0.2761605775441243\n",
      "6113\n",
      "0.2761605775441243\n",
      "6114\n",
      "0.2761605775441243\n",
      "6115\n",
      "0.2761605775441243\n",
      "6116\n",
      "0.2761605775441243\n",
      "6117\n",
      "0.2761605775441243\n",
      "6118\n",
      "0.2761605775441243\n",
      "6119\n",
      "0.2761605775441243\n",
      "6120\n",
      "0.2761605775441243\n",
      "6121\n",
      "0.2761605775441243\n",
      "6122\n",
      "0.2761605775441243\n",
      "6123\n",
      "0.2761605775441243\n",
      "6124\n",
      "0.2761605775441243\n",
      "6125\n",
      "0.2761605775441243\n",
      "6126\n",
      "0.2761605775441243\n",
      "6127\n",
      "0.2761605775441243\n",
      "6128\n",
      "0.2761605775441243\n",
      "6129\n",
      "0.2761605775441243\n",
      "6130\n",
      "0.2761605775441243\n",
      "6131\n",
      "0.2761605775441243\n",
      "6132\n",
      "0.2761605775441243\n",
      "6133\n",
      "0.2761605775441243\n",
      "6134\n",
      "0.2761605775441243\n",
      "6135\n",
      "0.2761605775441243\n",
      "6136\n",
      "0.2761605775441243\n",
      "6137\n",
      "0.2761605775441243\n",
      "6138\n",
      "0.2761605775441243\n",
      "6139\n",
      "0.2761605775441243\n",
      "6140\n",
      "0.2761605775441243\n",
      "6141\n",
      "0.2761605775441243\n",
      "6142\n",
      "0.2761605775441243\n",
      "6143\n",
      "0.2761605775441243\n",
      "6144\n",
      "0.2761605775441243\n",
      "6145\n",
      "0.2761605775441243\n",
      "6146\n",
      "0.2761605775441243\n",
      "6147\n",
      "0.2761605775441243\n",
      "6148\n",
      "0.2761605775441243\n",
      "6149\n",
      "0.2761605775441243\n",
      "6150\n",
      "0.2761605775441243\n",
      "9024\n",
      "0.22484642282970194\n",
      "9025\n",
      "0.22484642282970194\n",
      "9026\n",
      "0.22484642282970194\n",
      "9027\n",
      "0.22484642282970194\n",
      "9028\n",
      "0.22484642282970194\n",
      "9029\n",
      "0.22484642282970194\n",
      "9030\n",
      "0.22484642282970194\n",
      "9031\n",
      "0.22484642282970194\n",
      "9032\n",
      "0.22484642282970194\n",
      "9033\n",
      "0.22484642282970194\n",
      "9034\n",
      "0.22484642282970194\n",
      "9035\n",
      "0.22484642282970194\n",
      "9036\n",
      "0.22484642282970194\n",
      "9037\n",
      "0.22484642282970194\n",
      "9038\n",
      "0.22484642282970194\n",
      "9039\n",
      "0.22484642282970194\n",
      "9040\n",
      "0.22484642282970194\n",
      "9041\n",
      "0.22484642282970194\n",
      "9042\n",
      "0.22484642282970194\n",
      "9043\n",
      "0.22484642282970194\n",
      "9044\n",
      "0.22484642282970194\n",
      "9045\n",
      "0.22484642282970194\n",
      "9046\n",
      "0.22484642282970194\n",
      "9047\n",
      "0.22484642282970194\n",
      "9048\n",
      "0.22484642282970194\n",
      "9049\n",
      "0.22484642282970194\n",
      "9050\n",
      "0.22484642282970194\n",
      "9051\n",
      "0.22484642282970194\n",
      "9052\n",
      "0.22484642282970194\n",
      "9053\n",
      "0.22484642282970194\n",
      "9054\n",
      "0.22484642282970194\n",
      "9055\n",
      "0.22484642282970194\n",
      "9056\n",
      "0.22484642282970194\n",
      "9057\n",
      "0.22484642282970194\n",
      "9058\n",
      "0.22484642282970194\n",
      "9059\n",
      "0.22484642282970194\n",
      "9060\n",
      "0.22484642282970194\n",
      "9061\n",
      "0.22484642282970194\n",
      "9062\n",
      "0.22484642282970194\n",
      "9063\n",
      "0.22484642282970194\n",
      "9064\n",
      "0.22484642282970194\n",
      "9065\n",
      "0.22484642282970194\n",
      "9066\n",
      "0.22484642282970194\n",
      "9067\n",
      "0.22484642282970194\n",
      "9068\n",
      "0.22484642282970194\n",
      "9069\n",
      "0.22484642282970194\n",
      "9070\n",
      "0.22484642282970194\n",
      "9071\n",
      "0.22484642282970194\n",
      "9072\n",
      "0.22484642282970194\n",
      "9073\n",
      "0.22484642282970194\n",
      "9074\n",
      "0.22484642282970194\n",
      "9075\n",
      "0.22484642282970194\n",
      "9076\n",
      "0.22484642282970194\n",
      "9077\n",
      "0.22484642282970194\n",
      "9078\n",
      "0.22484642282970194\n",
      "9079\n",
      "0.22484642282970194\n",
      "9080\n",
      "0.22484642282970194\n",
      "9081\n",
      "0.22484642282970194\n",
      "9082\n",
      "0.22484642282970194\n",
      "9083\n",
      "0.22484642282970194\n",
      "9084\n",
      "0.22484642282970194\n",
      "9085\n",
      "0.22484642282970194\n",
      "9086\n",
      "0.22484642282970194\n",
      "9087\n",
      "0.22484642282970194\n",
      "9088\n",
      "0.22484642282970194\n",
      "9089\n",
      "0.22484642282970194\n",
      "9090\n",
      "0.22484642282970194\n",
      "9091\n",
      "0.22484642282970194\n",
      "9092\n",
      "0.22484642282970194\n",
      "9093\n",
      "0.22484642282970194\n",
      "9094\n",
      "0.22484642282970194\n",
      "9095\n",
      "0.22484642282970194\n",
      "9096\n",
      "0.22484642282970194\n",
      "9097\n",
      "0.22484642282970194\n",
      "9098\n",
      "0.22484642282970194\n",
      "9099\n",
      "0.22484642282970194\n",
      "9100\n",
      "0.22484642282970194\n",
      "9101\n",
      "0.22484642282970194\n",
      "9102\n",
      "0.22484642282970194\n",
      "9103\n",
      "0.22484642282970194\n",
      "9104\n",
      "0.22484642282970194\n",
      "9105\n",
      "0.22484642282970194\n",
      "9106\n",
      "0.22484642282970194\n",
      "9107\n",
      "0.22484642282970194\n",
      "9108\n",
      "0.22484642282970194\n",
      "9109\n",
      "0.22484642282970194\n",
      "9110\n",
      "0.22484642282970194\n",
      "9111\n",
      "0.22484642282970194\n",
      "9112\n",
      "0.22484642282970194\n",
      "9113\n",
      "0.22484642282970194\n",
      "9114\n",
      "0.22484642282970194\n",
      "9115\n",
      "0.22484642282970194\n",
      "9116\n",
      "0.22484642282970194\n",
      "9117\n",
      "0.22484642282970194\n",
      "9118\n",
      "0.22484642282970194\n",
      "9119\n",
      "0.22484642282970194\n",
      "9120\n",
      "0.22484642282970194\n",
      "9121\n",
      "0.22484642282970194\n",
      "9122\n",
      "0.22484642282970194\n",
      "9123\n",
      "0.22484642282970194\n",
      "9124\n",
      "0.22484642282970194\n",
      "9125\n",
      "0.22484642282970194\n",
      "9126\n",
      "0.22484642282970194\n",
      "9127\n",
      "0.22484642282970194\n",
      "9128\n",
      "0.22484642282970194\n",
      "9129\n",
      "0.22484642282970194\n",
      "9130\n",
      "0.22484642282970194\n",
      "9131\n",
      "0.22484642282970194\n",
      "9132\n",
      "0.22484642282970194\n",
      "9133\n",
      "0.22484642282970194\n",
      "9134\n",
      "0.22484642282970194\n",
      "9135\n",
      "0.22484642282970194\n",
      "9136\n",
      "0.22484642282970194\n",
      "9137\n",
      "0.22484642282970194\n",
      "9138\n",
      "0.22484642282970194\n",
      "9139\n",
      "0.22484642282970194\n",
      "9140\n",
      "0.22484642282970194\n",
      "9141\n",
      "0.22484642282970194\n",
      "9142\n",
      "0.22484642282970194\n",
      "9143\n",
      "0.22484642282970194\n",
      "9144\n",
      "0.22484642282970194\n",
      "9145\n",
      "0.22484642282970194\n",
      "9146\n",
      "0.22484642282970194\n",
      "9147\n",
      "0.22484642282970194\n",
      "9148\n",
      "0.22484642282970194\n",
      "9149\n",
      "0.22484642282970194\n",
      "9150\n",
      "0.22484642282970194\n",
      "9151\n",
      "0.22484642282970194\n",
      "9152\n",
      "0.22484642282970194\n",
      "9153\n",
      "0.22484642282970194\n",
      "9154\n",
      "0.22484642282970194\n",
      "9155\n",
      "0.22484642282970194\n",
      "9156\n",
      "0.22484642282970194\n",
      "9157\n",
      "0.22484642282970194\n",
      "9158\n",
      "0.22484642282970194\n",
      "9159\n",
      "0.22484642282970194\n",
      "9160\n",
      "0.22484642282970194\n",
      "9161\n",
      "0.22484642282970194\n",
      "9162\n",
      "0.22484642282970194\n",
      "9163\n",
      "0.22484642282970194\n",
      "9164\n",
      "0.22484642282970194\n",
      "9165\n",
      "0.22484642282970194\n",
      "9166\n",
      "0.22484642282970194\n",
      "9167\n",
      "0.22484642282970194\n",
      "9168\n",
      "0.22484642282970194\n",
      "9169\n",
      "0.22484642282970194\n",
      "9170\n",
      "0.22484642282970194\n",
      "9171\n",
      "0.22484642282970194\n",
      "9172\n",
      "0.22484642282970194\n",
      "9173\n",
      "0.22484642282970194\n",
      "9174\n",
      "0.22484642282970194\n",
      "9175\n",
      "0.22484642282970194\n",
      "9176\n",
      "0.22484642282970194\n",
      "9177\n",
      "0.22484642282970194\n",
      "9178\n",
      "0.22484642282970194\n",
      "9179\n",
      "0.22484642282970194\n",
      "9180\n",
      "0.22484642282970194\n",
      "9181\n",
      "0.22484642282970194\n",
      "9182\n",
      "0.22484642282970194\n",
      "9183\n",
      "0.22484642282970194\n",
      "9184\n",
      "0.22484642282970194\n",
      "9185\n",
      "0.22484642282970194\n",
      "9186\n",
      "0.22484642282970194\n",
      "9187\n",
      "0.22484642282970194\n",
      "9188\n",
      "0.22484642282970194\n",
      "9189\n",
      "0.22484642282970194\n",
      "9190\n",
      "0.22484642282970194\n",
      "9191\n",
      "0.22484642282970194\n",
      "9192\n",
      "0.22484642282970194\n",
      "9193\n",
      "0.22484642282970194\n",
      "9194\n",
      "0.22484642282970194\n",
      "9195\n",
      "0.22484642282970194\n",
      "9196\n",
      "0.22484642282970194\n",
      "9197\n",
      "0.22484642282970194\n",
      "9198\n",
      "0.22484642282970194\n",
      "9199\n",
      "0.22484642282970194\n",
      "9200\n",
      "0.22484642282970194\n",
      "9201\n",
      "0.22484642282970194\n",
      "9202\n",
      "0.22484642282970194\n",
      "9203\n",
      "0.22484642282970194\n",
      "9204\n",
      "0.22484642282970194\n",
      "9205\n",
      "0.22484642282970194\n",
      "9206\n",
      "0.22484642282970194\n",
      "9207\n",
      "0.22484642282970194\n",
      "9208\n",
      "0.22484642282970194\n",
      "9209\n",
      "0.22484642282970194\n",
      "9210\n",
      "0.22484642282970194\n",
      "9211\n",
      "0.22484642282970194\n",
      "9212\n",
      "0.22484642282970194\n",
      "9213\n",
      "0.22484642282970194\n",
      "9214\n",
      "0.22484642282970194\n",
      "9215\n",
      "0.22484642282970194\n",
      "9216\n",
      "0.22484642282970194\n",
      "9217\n",
      "0.22484642282970194\n",
      "9218\n",
      "0.22484642282970194\n",
      "9219\n",
      "0.22484642282970194\n",
      "9220\n",
      "0.22484642282970194\n",
      "9221\n",
      "0.22484642282970194\n",
      "9222\n",
      "0.22484642282970194\n",
      "9223\n",
      "0.22484642282970194\n",
      "9224\n",
      "0.22484642282970194\n",
      "9225\n",
      "0.22484642282970194\n",
      "9226\n",
      "0.22484642282970194\n",
      "9227\n",
      "0.22484642282970194\n",
      "9228\n",
      "0.22484642282970194\n",
      "9229\n",
      "0.22484642282970194\n",
      "9230\n",
      "0.22484642282970194\n",
      "9231\n",
      "0.22484642282970194\n",
      "9232\n",
      "0.22484642282970194\n",
      "9233\n",
      "0.22484642282970194\n",
      "9234\n",
      "0.22484642282970194\n",
      "9235\n",
      "0.22484642282970194\n",
      "9236\n",
      "0.22484642282970194\n",
      "9237\n",
      "0.22484642282970194\n",
      "9238\n",
      "0.22484642282970194\n",
      "9239\n",
      "0.22484642282970194\n",
      "9240\n",
      "0.22484642282970194\n",
      "9241\n",
      "0.22484642282970194\n",
      "9242\n",
      "0.22484642282970194\n",
      "9243\n",
      "0.22484642282970194\n",
      "9244\n",
      "0.22484642282970194\n",
      "9245\n",
      "0.22484642282970194\n",
      "9246\n",
      "0.22484642282970194\n",
      "9247\n",
      "0.22484642282970194\n",
      "9248\n",
      "0.22484642282970194\n",
      "9249\n",
      "0.22484642282970194\n",
      "9250\n",
      "0.22484642282970194\n",
      "9251\n",
      "0.22484642282970194\n",
      "9252\n",
      "0.22484642282970194\n",
      "9253\n",
      "0.22484642282970194\n",
      "9254\n",
      "0.22484642282970194\n",
      "9255\n",
      "0.22484642282970194\n",
      "9256\n",
      "0.22484642282970194\n",
      "9257\n",
      "0.22484642282970194\n",
      "9258\n",
      "0.22484642282970194\n",
      "9259\n",
      "0.22484642282970194\n",
      "9260\n",
      "0.22484642282970194\n",
      "9261\n",
      "0.22484642282970194\n",
      "9262\n",
      "0.22484642282970194\n",
      "9263\n",
      "0.22484642282970194\n",
      "9264\n",
      "0.22484642282970194\n",
      "9265\n",
      "0.22484642282970194\n",
      "9266\n",
      "0.22484642282970194\n",
      "9267\n",
      "0.22484642282970194\n",
      "9268\n",
      "0.22484642282970194\n",
      "9269\n",
      "0.22484642282970194\n",
      "9270\n",
      "0.22484642282970194\n",
      "9271\n",
      "0.22484642282970194\n",
      "9272\n",
      "0.22484642282970194\n",
      "9273\n",
      "0.22484642282970194\n",
      "9274\n",
      "0.22484642282970194\n",
      "9275\n",
      "0.22484642282970194\n",
      "9276\n",
      "0.22484642282970194\n",
      "9277\n",
      "0.22484642282970194\n",
      "9278\n",
      "0.22484642282970194\n",
      "9279\n",
      "0.22484642282970194\n",
      "9280\n",
      "0.22484642282970194\n",
      "9281\n",
      "0.22484642282970194\n",
      "9282\n",
      "0.22484642282970194\n",
      "9283\n",
      "0.22484642282970194\n",
      "9284\n",
      "0.22484642282970194\n",
      "9285\n",
      "0.22484642282970194\n",
      "9286\n",
      "0.22484642282970194\n",
      "9287\n",
      "0.22484642282970194\n",
      "9288\n",
      "0.22484642282970194\n",
      "9289\n",
      "0.22484642282970194\n",
      "9290\n",
      "0.22484642282970194\n",
      "9291\n",
      "0.22484642282970194\n",
      "9292\n",
      "0.22484642282970194\n",
      "9293\n",
      "0.22484642282970194\n",
      "9294\n",
      "0.22484642282970194\n",
      "9295\n",
      "0.22484642282970194\n",
      "9296\n",
      "0.22484642282970194\n",
      "9297\n",
      "0.22484642282970194\n",
      "9298\n",
      "0.22484642282970194\n",
      "9299\n",
      "0.22484642282970194\n",
      "9300\n",
      "0.22484642282970194\n",
      "9301\n",
      "0.22484642282970194\n",
      "9302\n",
      "0.22484642282970194\n",
      "9303\n",
      "0.22484642282970194\n",
      "9304\n",
      "0.22484642282970194\n",
      "9305\n",
      "0.22484642282970194\n",
      "9306\n",
      "0.22484642282970194\n",
      "9307\n",
      "0.22484642282970194\n",
      "9308\n",
      "0.22484642282970194\n",
      "9309\n",
      "0.22484642282970194\n",
      "9310\n",
      "0.22484642282970194\n",
      "9311\n",
      "0.22484642282970194\n",
      "9312\n",
      "0.22484642282970194\n",
      "9313\n",
      "0.22484642282970194\n",
      "9314\n",
      "0.22484642282970194\n",
      "9315\n",
      "0.22484642282970194\n",
      "12773\n",
      "0.2640751308410325\n",
      "12774\n",
      "0.2640751308410325\n",
      "12775\n",
      "0.2640751308410325\n",
      "12776\n",
      "0.2640751308410325\n",
      "12777\n",
      "0.2640751308410325\n",
      "12778\n",
      "0.2640751308410325\n",
      "12779\n",
      "0.2640751308410325\n",
      "12780\n",
      "0.2640751308410325\n",
      "12781\n",
      "0.2640751308410325\n",
      "12782\n",
      "0.2640751308410325\n",
      "12783\n",
      "0.2640751308410325\n",
      "12784\n",
      "0.2640751308410325\n",
      "12785\n",
      "0.2640751308410325\n",
      "12786\n",
      "0.2640751308410325\n",
      "12787\n",
      "0.2640751308410325\n",
      "12788\n",
      "0.2640751308410325\n",
      "12789\n",
      "0.2640751308410325\n",
      "12790\n",
      "0.2640751308410325\n",
      "12791\n",
      "0.2640751308410325\n",
      "12792\n",
      "0.2640751308410325\n",
      "12793\n",
      "0.2640751308410325\n",
      "12794\n",
      "0.2640751308410325\n",
      "12795\n",
      "0.2640751308410325\n",
      "12796\n",
      "0.2640751308410325\n",
      "12797\n",
      "0.2640751308410325\n",
      "12798\n",
      "0.2640751308410325\n",
      "12799\n",
      "0.2640751308410325\n",
      "12800\n",
      "0.2640751308410325\n",
      "12801\n",
      "0.2640751308410325\n",
      "12802\n",
      "0.2640751308410325\n",
      "12803\n",
      "0.2640751308410325\n",
      "12804\n",
      "0.2640751308410325\n",
      "12805\n",
      "0.2640751308410325\n",
      "12806\n",
      "0.2640751308410325\n",
      "12807\n",
      "0.2640751308410325\n",
      "12808\n",
      "0.2640751308410325\n",
      "12809\n",
      "0.2640751308410325\n",
      "12810\n",
      "0.2640751308410325\n",
      "12811\n",
      "0.2640751308410325\n",
      "12812\n",
      "0.2640751308410325\n",
      "12813\n",
      "0.2640751308410325\n",
      "12814\n",
      "0.2640751308410325\n",
      "12815\n",
      "0.2640751308410325\n",
      "12816\n",
      "0.2640751308410325\n",
      "12817\n",
      "0.2640751308410325\n",
      "12818\n",
      "0.2640751308410325\n",
      "12819\n",
      "0.2640751308410325\n",
      "12820\n",
      "0.2640751308410325\n",
      "12821\n",
      "0.2640751308410325\n",
      "12822\n",
      "0.2640751308410325\n",
      "12823\n",
      "0.2640751308410325\n",
      "12824\n",
      "0.2640751308410325\n",
      "12825\n",
      "0.2640751308410325\n",
      "12826\n",
      "0.2640751308410325\n",
      "12827\n",
      "0.2640751308410325\n",
      "12828\n",
      "0.2640751308410325\n",
      "12829\n",
      "0.2640751308410325\n",
      "12830\n",
      "0.2640751308410325\n",
      "12831\n",
      "0.2640751308410325\n",
      "12832\n",
      "0.2640751308410325\n",
      "12833\n",
      "0.2640751308410325\n",
      "12834\n",
      "0.2640751308410325\n",
      "12835\n",
      "0.2640751308410325\n",
      "12836\n",
      "0.2640751308410325\n",
      "12837\n",
      "0.2640751308410325\n",
      "12838\n",
      "0.2640751308410325\n",
      "12839\n",
      "0.2640751308410325\n",
      "12840\n",
      "0.2640751308410325\n",
      "12841\n",
      "0.2640751308410325\n",
      "12842\n",
      "0.2640751308410325\n",
      "12843\n",
      "0.2640751308410325\n",
      "12844\n",
      "0.2640751308410325\n",
      "12845\n",
      "0.2640751308410325\n",
      "12846\n",
      "0.2640751308410325\n",
      "12847\n",
      "0.2640751308410325\n",
      "12848\n",
      "0.2640751308410325\n",
      "12849\n",
      "0.2640751308410325\n",
      "12850\n",
      "0.2640751308410325\n",
      "12851\n",
      "0.2640751308410325\n",
      "12852\n",
      "0.2640751308410325\n",
      "12853\n",
      "0.2640751308410325\n",
      "12854\n",
      "0.2640751308410325\n",
      "12855\n",
      "0.2640751308410325\n",
      "12856\n",
      "0.2640751308410325\n",
      "12857\n",
      "0.2640751308410325\n",
      "12858\n",
      "0.2640751308410325\n",
      "12859\n",
      "0.2640751308410325\n",
      "12860\n",
      "0.2640751308410325\n",
      "12861\n",
      "0.2640751308410325\n",
      "12862\n",
      "0.2640751308410325\n",
      "12863\n",
      "0.2640751308410325\n",
      "12864\n",
      "0.2640751308410325\n",
      "12865\n",
      "0.2640751308410325\n",
      "12866\n",
      "0.2640751308410325\n",
      "12867\n",
      "0.2640751308410325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f14fc965af0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/beau/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\", line 4232, in <genexpr>\n",
      "    ta.write(time, out) for ta, out in zip(output_ta_t, flat_output))  File \"/home/beau/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py\", line 235, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "12868\n",
      "0.2640751308410325\n",
      "12869\n",
      "0.2640751308410325\n",
      "12870\n",
      "0.2640751308410325\n",
      "12871\n",
      "0.2640751308410325\n",
      "12872\n",
      "0.2640751308410325\n",
      "12873\n",
      "0.2640751308410325\n",
      "12874\n",
      "0.2640751308410325\n",
      "12875\n",
      "0.2640751308410325\n",
      "12876\n",
      "0.2640751308410325\n",
      "12877\n",
      "0.2640751308410325\n",
      "12878\n",
      "0.2640751308410325\n",
      "12879\n",
      "0.2640751308410325\n",
      "12880\n",
      "0.2640751308410325\n",
      "12881\n",
      "0.2640751308410325\n",
      "12882\n",
      "0.2640751308410325\n",
      "12883\n",
      "0.2640751308410325\n",
      "12884\n",
      "0.2640751308410325\n",
      "12885\n",
      "0.2640751308410325\n",
      "12886\n",
      "0.2640751308410325\n",
      "12887\n",
      "0.2640751308410325\n",
      "12888\n",
      "0.2640751308410325\n",
      "12889\n",
      "0.2640751308410325\n",
      "12890\n",
      "0.2640751308410325\n",
      "12891\n",
      "0.2640751308410325\n",
      "12892\n",
      "0.2640751308410325\n",
      "12893\n",
      "0.2640751308410325\n",
      "12894\n",
      "0.2640751308410325\n",
      "12895\n",
      "0.2640751308410325\n",
      "12896\n",
      "0.2640751308410325\n",
      "12897\n",
      "0.2640751308410325\n",
      "12898\n",
      "0.2640751308410325\n",
      "12899\n",
      "0.2640751308410325\n",
      "12900\n",
      "0.2640751308410325\n",
      "12901\n",
      "0.2640751308410325\n",
      "12902\n",
      "0.2640751308410325\n",
      "12903\n",
      "0.2640751308410325\n",
      "12904\n",
      "0.2640751308410325\n",
      "12905\n",
      "0.2640751308410325\n",
      "12906\n",
      "0.2640751308410325\n",
      "12907\n",
      "0.2640751308410325\n",
      "12908\n",
      "0.2640751308410325\n",
      "12909\n",
      "0.2640751308410325\n",
      "12910\n",
      "0.2640751308410325\n",
      "12911\n",
      "0.2640751308410325\n",
      "12912\n",
      "0.2640751308410325\n",
      "12913\n",
      "0.2640751308410325\n",
      "12914\n",
      "0.2640751308410325\n",
      "12915\n",
      "0.2640751308410325\n",
      "12916\n",
      "0.2640751308410325\n",
      "12917\n",
      "0.2640751308410325\n",
      "12918\n",
      "0.2640751308410325\n",
      "12919\n",
      "0.2640751308410325\n",
      "12920\n",
      "0.2640751308410325\n",
      "12921\n",
      "0.2640751308410325\n",
      "12922\n",
      "0.2640751308410325\n",
      "12923\n",
      "0.2640751308410325\n",
      "12924\n",
      "0.2640751308410325\n",
      "12925\n",
      "0.2640751308410325\n",
      "12926\n",
      "0.2640751308410325\n",
      "12927\n",
      "0.2640751308410325\n",
      "12928\n",
      "0.2640751308410325\n",
      "12929\n",
      "0.2640751308410325\n",
      "12930\n",
      "0.2640751308410325\n",
      "12931\n",
      "0.2640751308410325\n",
      "12932\n",
      "0.2640751308410325\n",
      "12933\n",
      "0.2640751308410325\n",
      "12934\n",
      "0.2640751308410325\n",
      "12935\n",
      "0.2640751308410325\n",
      "12936\n",
      "0.2640751308410325\n",
      "12937\n",
      "0.2640751308410325\n",
      "12938\n",
      "0.2640751308410325\n",
      "12939\n",
      "0.2640751308410325\n",
      "12940\n",
      "0.2640751308410325\n",
      "12941\n",
      "0.2640751308410325\n",
      "12942\n",
      "0.2640751308410325\n",
      "12943\n",
      "0.2640751308410325\n",
      "12944\n",
      "0.2640751308410325\n",
      "12945\n",
      "0.2640751308410325\n",
      "12946\n",
      "0.2640751308410325\n",
      "12947\n",
      "0.2640751308410325\n",
      "12948\n",
      "0.2640751308410325\n",
      "12949\n",
      "0.2640751308410325\n",
      "12950\n",
      "0.2640751308410325\n",
      "12951\n",
      "0.2640751308410325\n",
      "12952\n",
      "0.2640751308410325\n",
      "12953\n",
      "0.2640751308410325\n",
      "12954\n",
      "0.2640751308410325\n",
      "12955\n",
      "0.2640751308410325\n",
      "12956\n",
      "0.2640751308410325\n",
      "12957\n",
      "0.2640751308410325\n",
      "12958\n",
      "0.2640751308410325\n",
      "12959\n",
      "0.2640751308410325\n",
      "12960\n",
      "0.2640751308410325\n",
      "12961\n",
      "0.2640751308410325\n",
      "12962\n",
      "0.2640751308410325\n",
      "12963\n",
      "0.2640751308410325\n",
      "12964\n",
      "0.2640751308410325\n",
      "12965\n",
      "0.2640751308410325\n",
      "12966\n",
      "0.2640751308410325\n",
      "12967\n",
      "0.2640751308410325\n",
      "12968\n",
      "0.2640751308410325\n",
      "12969\n",
      "0.2640751308410325\n",
      "12970\n",
      "0.2640751308410325\n",
      "12971\n",
      "0.2640751308410325\n",
      "12972\n",
      "0.2640751308410325\n",
      "12973\n",
      "0.2640751308410325\n",
      "12974\n",
      "0.2640751308410325\n",
      "12975\n",
      "0.2640751308410325\n",
      "12976\n",
      "0.2640751308410325\n",
      "12977\n",
      "0.2640751308410325\n",
      "12978\n",
      "0.2640751308410325\n",
      "12979\n",
      "0.2640751308410325\n",
      "12980\n",
      "0.2640751308410325\n",
      "12981\n",
      "0.2640751308410325\n",
      "12982\n",
      "0.2640751308410325\n",
      "12983\n",
      "0.2640751308410325\n",
      "12984\n",
      "0.2640751308410325\n",
      "12985\n",
      "0.2640751308410325\n",
      "12986\n",
      "0.2640751308410325\n",
      "12987\n",
      "0.2640751308410325\n",
      "12988\n",
      "0.2640751308410325\n",
      "12989\n",
      "0.2640751308410325\n",
      "12990\n",
      "0.2640751308410325\n",
      "12991\n",
      "0.2640751308410325\n",
      "12992\n",
      "0.2640751308410325\n",
      "12993\n",
      "0.2640751308410325\n",
      "12994\n",
      "0.2640751308410325\n",
      "12995\n",
      "0.2640751308410325\n",
      "12996\n",
      "0.2640751308410325\n",
      "12997\n",
      "0.2640751308410325\n",
      "12998\n",
      "0.2640751308410325\n",
      "12999\n",
      "0.2640751308410325\n",
      "13000\n",
      "0.2640751308410325\n",
      "13001\n",
      "0.2640751308410325\n",
      "13002\n",
      "0.2640751308410325\n",
      "13003\n",
      "0.2640751308410325\n",
      "13004\n",
      "0.2640751308410325\n",
      "13005\n",
      "0.2640751308410325\n",
      "13006\n",
      "0.2640751308410325\n",
      "13007\n",
      "0.2640751308410325\n",
      "13008\n",
      "0.2640751308410325\n",
      "13009\n",
      "0.2640751308410325\n",
      "13010\n",
      "0.2640751308410325\n",
      "13011\n",
      "0.2640751308410325\n",
      "13012\n",
      "0.2640751308410325\n",
      "13013\n",
      "0.2640751308410325\n",
      "13014\n",
      "0.2640751308410325\n",
      "13015\n",
      "0.2640751308410325\n",
      "13016\n",
      "0.2640751308410325\n",
      "13017\n",
      "0.2640751308410325\n",
      "13018\n",
      "0.2640751308410325\n",
      "13019\n",
      "0.2640751308410325\n",
      "13020\n",
      "0.2640751308410325\n",
      "13021\n",
      "0.2640751308410325\n",
      "13022\n",
      "0.2640751308410325\n",
      "13023\n",
      "0.2640751308410325\n",
      "13024\n",
      "0.2640751308410325\n",
      "13025\n",
      "0.2640751308410325\n",
      "13026\n",
      "0.2640751308410325\n",
      "13027\n",
      "0.2640751308410325\n",
      "13028\n",
      "0.2640751308410325\n",
      "13029\n",
      "0.2640751308410325\n",
      "13030\n",
      "0.2640751308410325\n",
      "13031\n",
      "0.2640751308410325\n",
      "13032\n",
      "0.2640751308410325\n",
      "13033\n",
      "0.2640751308410325\n",
      "13034\n",
      "0.2640751308410325\n",
      "13035\n",
      "0.2640751308410325\n",
      "13036\n",
      "0.2640751308410325\n",
      "13037\n",
      "0.2640751308410325\n",
      "13038\n",
      "0.2640751308410325\n",
      "13039\n",
      "0.2640751308410325\n",
      "13040\n",
      "0.2640751308410325\n",
      "13041\n",
      "0.2640751308410325\n",
      "13042\n",
      "0.2640751308410325\n",
      "13043\n",
      "0.2640751308410325\n",
      "13044\n",
      "0.2640751308410325\n",
      "13045\n",
      "0.2640751308410325\n",
      "13046\n",
      "0.2640751308410325\n",
      "13047\n",
      "0.2640751308410325\n",
      "13048\n",
      "0.2640751308410325\n",
      "13049\n",
      "0.2640751308410325\n",
      "13050\n",
      "0.2640751308410325\n",
      "13051\n",
      "0.2640751308410325\n",
      "13052\n",
      "0.2640751308410325\n",
      "13053\n",
      "0.2640751308410325\n",
      "13054\n",
      "0.2640751308410325\n",
      "13055\n",
      "0.2640751308410325\n",
      "13056\n",
      "0.2640751308410325\n",
      "13057\n",
      "0.2640751308410325\n",
      "13058\n",
      "0.2640751308410325\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(e_x_train_np)):\n",
    "    if e_x_train_np[i][0][-1] > .201093:\n",
    "        print(i)\n",
    "        print(e_x_train_np[i][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   5.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,   5.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,   7.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,  10.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  0.        ,  12.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  1.        ,  14.        , 241.37709154,   0.71572337,\n",
       "          0.30109293],\n",
       "       [  1.        ,  19.        , 241.37709154,   0.71572337,\n",
       "          0.30109293]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_x_train_np[1207]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle training data before each epoch?\n",
    "Reserve validation set for each epoch?\n",
    "Set up hyperparameter tuning?\n",
    "Setup simulation routine and predictions.\n",
    "Save models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
